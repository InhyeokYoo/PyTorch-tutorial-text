{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP FROM SCRATCH: TRANSLATION WITH A SEQUENCE TO SEQUENCE NETWORK AND ATTENTION.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN5bPurwWPD9NwB1qbHYq7+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/InhyeokYoo/PyTorch-tutorial-text/blob/master/NLP_FROM_SCRATCH_TRANSLATION_WITH_A_SEQUENCE_TO_SEQUENCE_NETWORK_AND_ATTENTION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMfnfck4R_Yy",
        "colab_type": "text"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iY-tyCeQHPaE",
        "colab_type": "text"
      },
      "source": [
        "> 원본은 [링크](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)를 확인해주세요.\n",
        "\n",
        "이번시간은 밑바닥부터 \"NLP 시작하기(NLP from scratch)\"의 세번째이자 마지막 시간입니다. 밑바닥부터 시작하는만큼 우리는 우리만의 클래스와 함수를 직접 만들었습니다. 이번 튜토리얼을 마친후에 바로 따라오는 세 개의 튜토리얼에서 *torchtext*가 이러한 전처리를 어떻게 다루는지를 배우게 될 것입니다.\n",
        "\n",
        "이번 프로젝트는 뉴럴넷이 불어를 영어로 번역하도록 가르치겠습니다.\n",
        "\n",
        "```\n",
        "[KEY: > input, = target, < output]\n",
        "\n",
        "> il est en train de peindre un tableau .\n",
        "= he is painting a picture .\n",
        "< he is painting a picture .\n",
        "\n",
        "> pourquoi ne pas essayer ce vin delicieux ?\n",
        "= why not try that delicious wine ?\n",
        "< why not try that delicious wine ?\n",
        "\n",
        "> elle n est pas poete mais romanciere .\n",
        "= she is not a poet but a novelist .\n",
        "< she not not a poet but a novelist .\n",
        "\n",
        "> vous etes trop maigre .\n",
        "= you re too skinny .\n",
        "< you re all alone .\n",
        "```\n",
        "\n",
        "...와 같이 다양한 수준의 성공을 갖게 됩니다.\n",
        "\n",
        "이는 두 개의 RNN이 함께 작동하여 한 시퀀스에서 다른 시퀀스로 변환하는 [sequence to sequence network](https://arxiv.org/abs/1409.3215)의 간단하지만 강력한 아이디어가 이를 가능케합니다. 인코더는 input sequence를 벡터로 압축하고, 디코더는 이 벡터를 새로운 시퀀스로 펼치게됩니다(unfold).\n",
        "\n",
        "![](https://pytorch.org/tutorials/_images/seq2seq.png)\n",
        "\n",
        "이 모델의 성능을 향상시키기 위해, 우리는 [attention mechanism](https://arxiv.org/abs/1409.0473)를 사용할 것입니다. Attention은 디코더로 하여금 input sequence의 특정 범위에 초점을 맞추도록 합니다.\n",
        "\n",
        "**Recommended Reading:**\n",
        "\n",
        "I assume you have at least installed PyTorch, know Python, and understand Tensors:\n",
        "\n",
        "- https://pytorch.org/ For installation instructions\n",
        "- Deep Learning with PyTorch: A 60 Minute Blitz to get started with PyTorch in general\n",
        "- Learning PyTorch with Examples for a wide and deep overview\n",
        "- PyTorch for Former Torch Users if you are former Lua Torch user\n",
        "\n",
        "It would also be useful to know about Sequence to Sequence networks and how they work:\n",
        "- Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\n",
        "- Sequence to Sequence Learning with Neural Networks\n",
        "- Neural Machine Translation by Jointly Learning to Align and Translate\n",
        "- A Neural Conversational Model\n",
        "\n",
        "You will also find the previous tutorials on NLP From Scratch: Classifying Names with a Character-Level RNN and NLP From Scratch: Generating Names with a Character-Level RNN helpful as those concepts are very similar to the Encoder and Decoder models, respectively.\n",
        "\n",
        "And for more, read the papers that introduced these topics:\n",
        "- Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\n",
        "- Sequence to Sequence Learning with Neural Networks\n",
        "- Neural Machine Translation by Jointly Learning to Align and Translate\n",
        "- A Neural Conversational Model\n",
        "\n",
        "**Requirements**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqSFT2eAHHiQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6637tbLnSHAE",
        "colab_type": "text"
      },
      "source": [
        "# Loading data files\n",
        "이번 프로젝트의 데이터는 불어에서 영어로 번역한 쌍이 수천개가 들어있습니다.\n",
        "\n",
        "[Open Data Stack Exchange의 질문](https://opendata.stackexchange.com/questions/3888/dataset-of-sentences-translated-into-many-languages)를 통해 open 번역 사이트 https://tatoeba.org/ 를 발견할 수 있었고, https://tatoeba.org/eng/downloads 를 통해 다운로드가 가능합니다. 더 좋은 방법으로는, 누군가가 언어 쌍을 개개의 텍스트 파일로 분리작업 해놓은 추가 작업을 해놓은게 있습니다: https://www.manythings.org/anki/\n",
        "\n",
        "영어에서 프랑스 쌍은 repo에 포함하기에 너무 크기 때문에, 계속하려면 `data/eng-fra.txt`를 다운받아야 합니다. 이 파일은 tab으로 언어 쌍이 분리되어있습니다.\n",
        "```\n",
        "I am cold.    J'ai froid.\n",
        "```\n",
        "\n",
        "**Note**\n",
        "다운로드는 [이곳](https://download.pytorch.org/tutorial/data.zip)에서 받을 수 있습니다.\n",
        "\n",
        "> 마찬가지로 밑에서 wget으로 받을 수 있게 해놨습니다.\n",
        "\n",
        "Chraceter-level RNN 튜토리얼과 마찬가지로, 비슷한 character encoding이 사용됩니다. 한 언어에 있는 각 단어를 one-hot vector나, 하나만 제외한(단어의 index) 모든 요소가 0인 거대한 벡터로 표현하겠습니다. 한 언어에 존재하는 글자와 비교하면, 단어는 매우 매우 많이 존재하기 때문에 encoding vector가 매우 큽니다. 그러나 우리는 데이터에 약간의 트릭과 잘라내기를 통해 한 언어당 몇 천개의 단어만 사용할 것입니다.\n",
        "\n",
        "![](https://pytorch.org/tutorials/_images/word-encoding.png)\n",
        "\n",
        "추후 네트워크의 input과 target으로 사용하기 위해 각 단어당 고유의 인덱스가 필요합니다. 이러한 과정을 기록하기 위해 `Lang`이라 불리는 helper class를 사용할 것입니다. 이는 word → index (`word2index`)와 index → word  (`index2word`) 딕셔너리를 포함하고, 또한, 희귀한 단어를 처리하기 위한 `word2count`를 포함합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABhf-_NnSGXu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "outputId": "8a4eb056-68b9-4cf6-b6f9-07302ae6f31b"
      },
      "source": [
        "!wget https://download.pytorch.org/tutorial/data.zip\n",
        "!unzip data.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-04 11:22:53--  https://download.pytorch.org/tutorial/data.zip\n",
            "Resolving download.pytorch.org (download.pytorch.org)... 54.192.151.21, 54.192.151.98, 54.192.151.109, ...\n",
            "Connecting to download.pytorch.org (download.pytorch.org)|54.192.151.21|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2882130 (2.7M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "\rdata.zip              0%[                    ]       0  --.-KB/s               \rdata.zip            100%[===================>]   2.75M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2020-02-04 11:22:53 (81.3 MB/s) - ‘data.zip’ saved [2882130/2882130]\n",
            "\n",
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "  inflating: data/eng-fra.txt        \n",
            "   creating: data/names/\n",
            "  inflating: data/names/Arabic.txt   \n",
            "  inflating: data/names/Chinese.txt  \n",
            "  inflating: data/names/Czech.txt    \n",
            "  inflating: data/names/Dutch.txt    \n",
            "  inflating: data/names/English.txt  \n",
            "  inflating: data/names/French.txt   \n",
            "  inflating: data/names/German.txt   \n",
            "  inflating: data/names/Greek.txt    \n",
            "  inflating: data/names/Irish.txt    \n",
            "  inflating: data/names/Italian.txt  \n",
            "  inflating: data/names/Japanese.txt  \n",
            "  inflating: data/names/Korean.txt   \n",
            "  inflating: data/names/Polish.txt   \n",
            "  inflating: data/names/Portuguese.txt  \n",
            "  inflating: data/names/Russian.txt  \n",
            "  inflating: data/names/Scottish.txt  \n",
            "  inflating: data/names/Spanish.txt  \n",
            "  inflating: data/names/Vietnamese.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9_MgA7lXvbO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0:'SOS', 1:'SOS'}\n",
        "        self.n_words = 2 # 단어의 갯수(SOS와 BOS)\n",
        "\n",
        "    def add_sentence(self, sentence):\n",
        "        # sentence내의 단어들을 저장함.\n",
        "        for word in sentence.split(' '):\n",
        "            self.add_word(word)\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzegE-93YvqW",
        "colab_type": "text"
      },
      "source": [
        "이 파일은 모두 Unicode로 저장되어 있습니다. 이를 간단하게 표현하기 위해 Unicode 글자를 ASCII로 표현하고, 구둣점을 잘라내고 소문자로 표현합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQ_yXQvrYo9l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )   # comprehension\n",
        "\n",
        "# 소문자, 잘라내기(trim), 글자가 아닌 character 잘라내기\n",
        "def normalize_string(s):\n",
        "    s = unicode_to_ascii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)   # The backreference \\1 (backslash one) references the first capturing group\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nIScqVL9F9Ye"
      },
      "source": [
        "데이터 파일을 읽기 위해 파일을 line으로 split하고, line을 쌍으로 split합니다. 파일은 영어 → 다른 언어로 이루어져있기 때문에 다른 언어 → 영어 번역을 원한다면 `reverse` flag를 추가하여 reverse 쌍을 추가합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARG8YskbbhDO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_langs(lang1, lang2, reverse=True):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # 파일을 읽고 line으로 분리합니다\n",
        "    lines = open(f'data/{lang1}-{lang2}.txt', encoding='utf-8').read().strip().split('\\n')\n",
        "\n",
        "    # 모든 line을 pair로 분리하고 normalize합니다.\n",
        "    pairs = [[normalize_string(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    # 쌍을 reverse하고, Lang instance를 만듭니다.\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijK_ZZny44ji",
        "colab_type": "text"
      },
      "source": [
        "여기에는 `많은` 예제 문장이 있고 빠르게 학습시키기를 원하기 때문에, 비교적 간편하고 간단하게 문장을 자르겠습니다. 여기서 최고 길이는 10입니다(마지막 구둣점을 포함). 또한, \"I am\"이나 \"He is\" 등과 같은 형태의 문장을 필터링합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d39u3Ld_4wvb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 10 # final\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "def filter_pair(p):\n",
        "    # 문장 둘 다 단어수가 10개 이하고, eng_prefixes로 시작하는 문장만 고름.\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "    len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "    p[1].startswith(eng_prefixes)\n",
        "\n",
        "def filter_pairs(pairs):\n",
        "    return [pair for pair in pairs if filter_pair(pair)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbdJlgao94d3",
        "colab_type": "text"
      },
      "source": [
        "데이터 전처리를 위한 모든 과정은 다음과 같습니다:\n",
        "- 텍스트 파일을 읽고 라인으로 분리한 뒤, 라인을 쌍으로 분리\n",
        "- 텍스트를 노말라이즈하고, 길이와 내용물로 필터링\n",
        "- 언어쌍의 문장들로 단어 리스트 만들기\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yGMy2w89319",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "4b86874d-7737-4b99-df18-346ce768bebf"
      },
      "source": [
        "def prepare_data(lang1, lang2, reverse=False):\n",
        "    # 텍스트 파일을 읽고 이를 라인으로 분리한 뒤, 라인을 쌍으로 분리\n",
        "    input_lang, output_lang, pairs = read_langs(lang1, lang2, reverse)\n",
        "    print(f\"Read {len(pairs)} sentence pairs\")\n",
        "\n",
        "    # 텍스트를 노말라이즈하고, 길이와 내용물로 필터링\n",
        "    pairs = filter_pairs(pairs)\n",
        "    print(f\"Trimmed to {len(pairs)} sentence pairs\")\n",
        "    print(\"Counting words...\")\n",
        "    \n",
        "    # 언어쌍의 문장들로 단어 리스트 만들기\n",
        "    for pair in pairs:\n",
        "        input_lang.add_sentence(pair[0])\n",
        "        output_lang.add_sentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "input_lang, output_lang, pairs = prepare_data('eng', 'fra', True)\n",
        "print(random.choice(pairs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 135842 sentence pairs\n",
            "Trimmed to 10599 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "fra 4345\n",
            "eng 2803\n",
            "['nous allons faire des courses .', 'we re going shopping .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CnM07XXGndA",
        "colab_type": "text"
      },
      "source": [
        "# The Seq2Seq Model\n",
        "RNN은 sequence에 동작하고 다음 단계의 입력으로 자신의 출력을 사용하는 네트워크입니다.\n",
        "\n",
        "Sequence to sequence network/seq2seq 네트워크/ Encoder Decoder 네트워크는 encoder와 decoder로 불리는 두 개의 RNN으로 이루어진 모델입니다. Encoder는 input sequnce를 읽고 하나의 벡터를 반환합니다. 디코더는 이 벡터를 읽고 output sequence를 반환합니다.\n",
        "\n",
        "![](https://pytorch.org/tutorials/_images/seq2seq.png)\n",
        "\n",
        "한 개의 RNN으로 모든 input이 output에 대응하는 sequence를 예측하는 것과는 달리, seq2seq model은 sequnce 길이와 순서에 자유롭습니다. 이는 두 개의 언어 사이를 번역하는 것에 매우 유용합니다.\n",
        "\n",
        "\"Je ne suis pas le chat noir\" → \"I am not the black cat\"와 같은 문장을 고려해봅시다. input 문장의 대부분의 단어는 output 문장의 직역이지만, \"chat noir\" 과 \"black cat\"와 같이 순서는 약간 다릅니다. \"ne/pas\"의 구성 때문에 input sequence에는 하나의 단어가 더 있습니다. 이는 input 단어로부터 직접 올바르게 번역하는 것을 어렵게 만듭니다.\n",
        "\n",
        "seq2seq에서 encoder는 최상의 시나리오 상 input sequence의 \"의미\"를 하나의 벡터로 압축합니다. 이는 문장의 N-dimensional space의 한 점을 가리킵니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbLsyBDZ49ks",
        "colab_type": "text"
      },
      "source": [
        "> 더 구체적인 설명\n",
        "\n",
        "Seq2seq은 품사 판별과 같은 sequential labeling과는 다릅니다. 이는 입력 단어열 [$x_1, x_2, ..., x_n$]의 각 $x_i$에 해당하는 [$y_1, y_2, ..., y_n$]을 출력하는 것으로, 같은 길이에 대해서만 동작합니다.\n",
        "\n",
        "Seq2seq가 학습하는 기준은 $maximizs \\sum P_\\theta(y_{1:m}|x_{1:n})$으로, $x_{1:n}$과 $y_{1:m}$의 상관성을 최대화하는 것입니다. 이때, seq2seq은 input sequence의 정보를 하나의 context c에 저장합니다. Encoder RNN의 마지막 hidden state vector는 $c$이고, Decoder는 고정된 context vector $c$와 현재까지 생성된 단어열 $y_{1:i-1}$을 이용하는 language model (sentence generator)입니다.\n",
        "\n",
        "즉, $P_\\theta(y_{1:m}|x_{1:n}) = \\Pi_i P_\\theta(y_i|y_{1:i-1}, c)$를 통해 학습하는 것입니다. 밑은 이를 도식화한 그림입니다.\n",
        "\n",
        "\n",
        "![](https://lovit.github.io/assets/figures/seq2seq_fixed_context.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeDDXsIWL41B",
        "colab_type": "text"
      },
      "source": [
        "## The Encoder\n",
        "seq2seq의 encoder는 input 문장의 모든 단어들을 위한 하나의 값을 내놓는 RNN입니다. 모든 input word에 대해 encoder는 하나의 벡터와 hidden state를 내놓습니다. 그리고 다음 단어를 위해 hidden state를 사용합니다.\n",
        "\n",
        "![](https://pytorch.org/tutorials/_images/encoder-network.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqH_l1OEFS4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        # input_size만큼 차원을 받고(즉, embedding할 단어의 개수, lookup table의 차원의 갯수)\n",
        "        # hidden_size크기의 vector를 반환함\n",
        "        self.embedding = nn.Embedding(num_embeddings=input_size, embedding_dim=hidden_size)\n",
        "        self.gru = nn.GRU(input_size=hidden_size, hidden_size=hidden_size)\n",
        "    \n",
        "    def forward(self, input, hidden):\n",
        "        # input: vector <Input_size>\n",
        "        embedded = self.embedding(input).view(1, 1, -1) # <1 x hidden_size> -> <1 x 1 x hidden_size>\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnJRtiG5OnhX",
        "colab_type": "text"
      },
      "source": [
        "## The Decoder\n",
        "디코더는 또 다른 RNN으로, 인코더의 아웃풋 벡터를 받고 단어들의 sequence가 번역을 만들도록 합니다.\n",
        "\n",
        "### Simple Decoder\n",
        "가장 단순한 seq2seq 디코더를 위해 우리는 인코더의 마지막 아웃풋만을 사용하겠습니다. 이 마지막 벡터는 때때로 *context vector*라고도 불리며, 말 그대로 전체 시퀀스를 하나의 context로 인코딩합니다. 초기 input token은 start-of-string `<SOS>`이고, 첫 hiddne state는 context vector입니다(인코더의 마지막 output을 의미).\n",
        "\n",
        "![](https://pytorch.org/tutorials/_images/decoder-network.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqf4Cx9aOnGF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # 번역 도착 언어의 embedding\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(input_size=hidden_size, hidden_size=hidden_size)\n",
        "        # output를 내놓는다는 점만 빼면 encoder와 동일\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)   # <1 x hidden_size> ->  <1 x 1 x hidden_size>\n",
        "        print(f\"decoder embedding size:{output.size()}\")\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8LSO1vOTmws",
        "colab_type": "text"
      },
      "source": [
        "## Attention Decoder\n",
        "만약 context 벡터만이 encoder와 decoder 사이를 지나간다면, 이 벡터는 문장 전체를 인코딩해야 하므로 부담이 많을 것입니다.\n",
        "\n",
        "Attention은 디코더만의 output의 모든 스텝에서 디코더로 하여금 인코더의 아웃풋의 각기 다른 부분에 \"주목\"하게 합니다. 가장 먼저 우리는 *attention weight*를 계산하게 됩니다. 이는 인코더의 아웃풋 벡터에 곱해져 weighted combination을 생성할 것입니다. 이 결과물(코드 상에선 `attn_applied`)은 인풋 시퀀스의 특정 부분에 대한 정보를 갖고 있을 것입니다. 따라서 이는 디코더로 하여금 올바른 아웃풋 단어를 선택하게끔 도와줍니다.\n",
        "\n",
        "![](https://i.imgur.com/1152PYf.png)\n",
        "\n",
        "어텐션 웨이트를 계산하는 것은 다른 FFN인 `attn`에서 디코더의 인풋과 hidden state를 input으로 이용하여 이루어집니다. 이는 트레이닝 셋에 모든 사이즈의 문장이 있기 때문에, 실제로 만들고 이 레이어를 훈련시키기 위해서는 적용할 수 있는 최대 문장 길이를 선택해야만 합니다(인코더 아웃풋에선 input length). 최대 길이의 문장은 모든 어텐션 웨이트를 사용하는 반면, 짧은 문장은 오로지 몇몇만 사용할 것입니다.\n",
        "\n",
        "![](https://pytorch.org/tutorials/_images/attention-decoder-network.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEYLnggDCbc6",
        "colab_type": "text"
      },
      "source": [
        "> 구체적인 설명\n",
        "\n",
        "Bahdanau et al., (2014) 에서 제시한 바에 따르면 seq2seq와 같이 하나의 문장에 대한 정보를 하나의 context vector $c$로 표현하는 것이 충분하지 않다고 문제를 제기합니다. 이는 Decoder RNN 이 문장을 만들 때 각 단어가 필요한 정보가 다를텐데, sequence to sequence 는 매 시점에 동일한 context $c$를 이용하기 때문입니다. 대신에 $x1, x_2, ..., x_n$에 해당하는 encoder RNN의 hidden state vector $h_1, h_2, ..., h_n$의 조합으로 $y_i$마다 다르게 조합하여 이용하는 방법을 제안합니다.\n",
        "\n",
        "![](https://lovit.github.io/assets/figures/seq2seq_with_attention.png)\n",
        "\n",
        "그림처럼 decoder RNN이 $y_i$를 선택할 때 encoder RNN의 $h_j$를 얼마나 사용할지를 $a_{ij}$로 정의합니다. $y_i$의 context vector $c_i$는 $\\sum_j a_{ij} \\cdot h_j$로 정의되며, $\\sum_j a_{ij} =1, a_{ij} >= 0$ 입니다. $a_{ij}$를 attention weight라 하며, 이 역시 neural network에 의하여 학습됩니다.\n",
        "\n",
        "Weight 는 decoder 의 이전 hidden state $s_i$와 encoder 의 hidden state $h_j$가 input으로 입력되는 feed-forward neural network 입니다. 출력값 $e_{ij}$는 하나의 숫자이며, 이들을 softmax 로 변환하여 확률 형식으로 표현합니다. 그리고 이 확률을 이용하여 encoder hidden vectors 의 weighted average vector 를 만들어 context vector $c_i$로 이용합니다.\n",
        "\n",
        "$$\n",
        "a_{ij} = \\frac{\\exp(e_{ij})}{\\sum_j \\exp (e_ij)}, e_{ij} = f(s_{i-1}, h_j)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHVZW0U3TcIb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        # embedding\n",
        "        embedded = self.embedding(input).view(1, 1, -1) # <1 x 1 x hidden_size>\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        # prev_hidden과 embedded input을 concat하여 attention weight a_ij를 계산\n",
        "        # embedded[0]: <1 x hidden_size>\n",
        "        # hidden[0]: <1 x hidden_size>\n",
        "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)  # <1 x hidden_size * 2> -> <1 x max_length>\n",
        "        # encoder_outputs: <max_length(10) x hidden_size>\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))   # -> <1 x 1 x hidden_size>\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)   # ->  <1 x hidden_size * 2>\n",
        "        output = self.attn_combine(output).unsqueeze(0) # -> <1 x 1 x hidden_size>\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYskdms9Yigc",
        "colab_type": "text"
      },
      "source": [
        "**Note**\n",
        "상대적 위치로 접근하여 길이 제한 문제를 해결한 다른 형태의 attention도 있습니다. [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025) 를 통해 \"Local attention\"에 대해 읽어보세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NJHgT6bZBab",
        "colab_type": "text"
      },
      "source": [
        "# Training\n",
        "## Preparing Training Data\n",
        "훈련시키기 위해서는 각 언어 쌍에 대해 우리는 인풋 텐서(인풋 문장에 대한 단어의 인덱스)와 타겟 텐서(타겟 문장에 대한 단어의 인덱스)가 필요할 것입니다. 이러한 벡터를 생성하고, EOS 토큰 또한 만들겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MG51yXa8Yee1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def index_from_sentence(lang, sentence):\n",
        "    # sentence 내 단어를 lang 인스턴스를 통해 index로 변환\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensor_from_sentence(lang, sentence):\n",
        "    # index_from_sentence를 통해 sentence내 단어의 indices를 얻음\n",
        "    indexes = index_from_sentence(lang, sentence)\n",
        "    # EOS 토큰 추가\n",
        "    indexes.append(EOS_token)\n",
        "    # 단어 인덱스로 이루어진 (1 x n_words) tensor 반환\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)   # <n_words + 1 x 1>\n",
        "\n",
        "def tensors_from_pair(pair):\n",
        "    # 번역 쌍을 tensor로 변환함: <input_length x 1>\n",
        "    input_tensor = tensor_from_sentence(input_lang, pair[0])\n",
        "    target_tensor = tensor_from_sentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0f4VbBBaQV4",
        "colab_type": "text"
      },
      "source": [
        "## Training the model\n",
        "학습시키기 위해 우리는 인코더를 통해 인풋 문장을 돌릴 것이고, 마지막 히든 스테이트와 매 순간의 아웃풋에 대해 추적할 것입니다. 그 후, 디코더는 `<SOS>` 토큰을 첫번째 인풋으로 받을 것이고, 인코더의 마지막 히든 스테이트는 디코더의 첫번째 히든 스테이트가 될 것입니다.\n",
        "\n",
        "*Teacher forcing*은 디코더의 예측을 다음 인풋으로 넣는 대신, 진짜 타겟을 매 다음 인풋으로 사용하는 것에 대한 개념입니다. Teacher forcing을 사용하는 것은 더 빠르게 수렴하도록 하지만, [학습 중인 네트워크가 잘못 사용될 때 불안전 할 수 있습니다.](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.378.4095&rep=rep1&type=pdf)\n",
        "\n",
        "tearch forcing된 네트워크의 아웃풋은 일관된 문법을 갖고 있지만 정확한 번역과는 거리가 좀 있습니다. 직관적으로 네트워크는 아웃풋 문법을 표현하는 것을 배우고 한번 교사가 처음 몇 단어를 얘기해주면 의미를 \"선택\"할 수 있지만 번역에서 처음부터 단어를 생성하는 일은 잘 못합니다.\n",
        "\n",
        "PyTorch의 autograd가 우리에게 주는 자유로움 덕분에, 간단한 if문만으로 teacher forcing을 사용할지 말지 결정할 수 있습니다.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HxA-oM9aP9K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.init_hidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    # 디코더에선 문장의 최대 길이를 정하여 계산\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)   # <10 x 256>\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: 다음 인풋에 target을 feed\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            # BPTT를 위해 더해만 줌.\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: prediction을 다음 인풋에 feed\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            # prediction을 뽑아냄.\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2YbGKWxJyzC",
        "colab_type": "text"
      },
      "source": [
        "이는 helper function으로 시간 경과와 진행도를 보여주는 함수입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgjhIgq5Jv_W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def as_minutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def time_since(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCelDS4kKIEv",
        "colab_type": "text"
      },
      "source": [
        "전체 프로세스는 다음 모양과 같습니다.\n",
        "- 타이머 시작\n",
        "- optimizer와 criterion 초기화\n",
        "- 학습 쌍 만들기\n",
        "- 빈 loss array를 만들어 plotting하기\n",
        "\n",
        "그 후 `train`을 많이 부르고 때때로 진척도(예제의 %, 걸린 시간, 남은 시간)와 average loss를 출력하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dvW7q1wKEcI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_iters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensors_from_pair(random.choice(pairs)) for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (time_since(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    show_plot(plot_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSV3NkpJLR_d",
        "colab_type": "text"
      },
      "source": [
        "## Plotting results\n",
        "Plotting은 matplotlib으로 진행되며, loss 값이 담긴 array를 `plot losses`를 이용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwbd08tIKI7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def show_plot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQNr0nfuLvU1",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation\n",
        "평가는 대부분 학습과정과 똑같습니다만 타겟이 없으므로 디코더의 결과값을 다시 인풋으로 넣어줘야 합니다. 매 순간 단어를 예측하고, 이를 output string에 추가하며, EOS 토큰이 나올 경우 멈추면 됩니다. 나중에 이를 그려보기 위해 디코더의 어텐션값을 저장해놓겠습니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE6X_xHhLuPb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensor_from_sentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.init_hidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)   # <10 x 256>\n",
        "        \n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        # 위의 Without Teacher Forcing 부분과 비슷함.\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiH3tCe6NQ_r",
        "colab_type": "text"
      },
      "source": [
        "이제 임의의 문장으로 평가하면 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bafX730jNPtH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_randomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOCRyzA4Nc0f",
        "colab_type": "text"
      },
      "source": [
        "# Training and Evaluating\n",
        "이러한 헬퍼 함수를 이용해서(이는 추가적인 작업처럼 보이지만, 여러개의 실험을 더 쉽게 실행할 수 있게끔 도와줍니다) 네트워크를 초기화하고 학습을 시작합니다.\n",
        "\n",
        "인풋 문장은 까다롭게 필터링 됐다는 점을 생각해보면 이렇게 작은 데이터셋으로는 상대적으로 작은 256의 히든 스테이트를 갖는 네트워크와 하나의 GRU 레이어를 사용합니다. 맥북 CPU 40분 정도 지나면, 꽤나 그럴듯한 결과물을 볼 수 있을겁니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boeH26KvNbwo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "409467e9-fc9c-4f9d-c40f-fc74b6de5bd7"
      },
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "train_iters(encoder1, attn_decoder1, 75000, print_every=5000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "encoder's output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n",
            "attn last output size: torch.Size([1, 1, 256])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-6a9567d233fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mattn_decoder1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttnDecoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_iters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_decoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m75000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-63-e9a74cf8aaec>\u001b[0m in \u001b[0;36mtrain_iters\u001b[0;34m(encoder, decoder, n_iters, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtarget_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mplot_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-90e3948994a3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Teacher forcing: 다음 인풋에 target을 feed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdi\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Teacher forcing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-68-77587113c3fa>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# 단어 개수만큼 되야되는거 아닌가?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"attn last output size: {output.size()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yBTLChLQKKF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluate_randomly(encoder1, attn_decoder1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdPgwKjAQS0_",
        "colab_type": "text"
      },
      "source": [
        "# Visualizing Attention\n",
        "어텐션 메카니즘의 유용한 성질은 매우 해석하기 쉬운 결과물을 내놓은단 겁니다. 이는 인풋 시퀀스의 특정 인코더 아웃풋에 가중치를 두어 네트워크가 매 시간에서 어디에 집중하는지 알 수 있습니다.\n",
        "\n",
        "그냥 `plt.matshow(attentions)`을 실행해 컬럼은 input step, 로우는 output step으로하는 어텐션의 매트릭스로 만들 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qbgqQDhQPBT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_words, attentions = evaluate(encoder1, attn_decoder1, \"je suis trop froid .\")\n",
        "plt.matshow(attentions.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1t4KGQrGhYOB",
        "colab_type": "text"
      },
      "source": [
        "더 나은 비쥬얼라이징을 위하여 축과 레이블을 추가하는 작업을 진행하겠습니다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsVrs_B1SI2k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_attention(input_sentence, output_words, attentions):\n",
        "    # Set up figure with colorbar\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def evaluate_and_show_attention(input_sentence):\n",
        "    output_words, attentions = evaluate(encoder1, attn_decoder1, input_sentence)\n",
        "    print('input =', input_sentence)\n",
        "    print('output =', ' '.join(output_words))\n",
        "    showAttention(input_sentence, output_words, attentions)\n",
        "\n",
        "\n",
        "evaluate_and_show_attention(\"elle a cinq ans de moins que moi .\")\n",
        "\n",
        "evaluate_and_show_attention(\"elle est trop petit .\")\n",
        "\n",
        "evaluate_and_show_attention(\"je ne crains pas de mourir .\")\n",
        "\n",
        "evaluate_and_show_attention(\"c est un jeune directeur plein de talent .\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBMFFWBmjdwe",
        "colab_type": "text"
      },
      "source": [
        "# Exercises\n",
        "- Try with a different dataset\n",
        " - Another language pair\n",
        " - Human → Machine (e.g. IOT commands)\n",
        " - Chat → Response\n",
        " - Question → Answer\n",
        "- Replace the embeddings with pre-trained word embeddings such as word2vec or GloVe\n",
        "- Try with more layers, more hidden units, and more sentences. Compare the training time and results.\n",
        "- If you use a translation file where pairs have two of the same phrase (I am test \\t I am test), you can use this as an autoencoder. Try this:\n",
        " - Train as an autoencoder\n",
        " - Save only the Encoder network\n",
        " - Train a new Decoder for translation from there"
      ]
    }
  ]
}