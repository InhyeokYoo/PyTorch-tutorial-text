{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SEQUENCE-TO-SEQUENCE MODELING WITH NN.TRANSFORMER AND TORCHTEXT.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNFT0N2GNaNv/lVG3LAevnJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/InhyeokYoo/PyTorch-tutorial-text/blob/master/SEQUENCE_TO_SEQUENCE_MODELING_WITH_NN_TRANSFORMER_AND_TORCHTEXT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jxBVEj8b_jY",
        "colab_type": "text"
      },
      "source": [
        "# Intro.\n",
        "\n",
        "이번 튜토리얼은 [nn.Transformer](https://pytorch.org/docs/master/nn.html?highlight=nn%20transformer#torch.nn.Transformer)를 이용하여 sequence-to-sequence 모델을 어떻게 학습시키는지 알아보겠습니다.\n",
        "\n",
        "Pytroch 1.2 release는 [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf)에 기반한 표준 transformer 모듈을 포함합니다. transformer 모델은 더욱 parallelizable하면서 다양한 sequence-to-sequence에 우월함이 증명되었습니다. `nn.Transformer`는 전적으로 attention mechanism(최근 [nn.MultiheadAttention](https://pytorch.org/docs/master/nn.html?highlight=multiheadattention#torch.nn.MultiheadAttention)으로 구현된 다른 모듈)에 의존하여 인풋과 아웃풋사이의 global dependency를 추출합니다. `nn.Transformer`은 고도로 모듈화되어 이 튜토리얼의 [nn.TransformerEncoder](https://pytorch.org/docs/master/nn.html?highlight=nn%20transformerencoder#torch.nn.TransformerEncoder)와 같은 단일 component가 쉽게 결합/적용될 수 있게합니다.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://pytorch.org/tutorials/_images/transformer_architecture.jpg\" height=600>\n",
        "</center>\n",
        "\n",
        "> 원문은 [다음](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)을 참고해주세요. 저번과 마찬가지로 제 개인적인 의견은 이렇게 citation으로 달겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPLCL6P-15ju",
        "colab_type": "text"
      },
      "source": [
        "# Define the model\n",
        "이번 튜토리얼에서는 language modeling 작업에 대해 `nn.TransformerEncoder`을 학습시켜보겠습니다. lanuage modeling 작업은 따라오는 a sequence of words에 대한 given word (혹은 sequence of words)의 그럴듯한 확률(probability for the likelihood)을 할당하는 것입니다. A sequence of tokens는 먼저 embedding layer로 전달된 후, 단어 순서에 대한 정보를 전달하기 위해 positional encoding layer로 전달됩니다. `nn.TransformerEncoder`는 여러개의 [nn.TransformerEncoderLayer](https://pytorch.org/docs/master/nn.html?highlight=transformerencoderlayer#torch.nn.TransformerEncoderLayer)으로 이루어져 있습니다. 인풋 시퀀스와 함께, *square attention mask*를 필요로 하는데, 이는 `nn.TransformerEncoder` 안의 self-attention layer가 오직 이전에 등장한 단어들에만 주목(attend)하도록 되어있기 때문입니다. language modeling 작업에서는 미래 위치에 있는 어떠한 토큰이던지 전부 가려집니다(maksed). `nn.TransformerEncoder`는 마지막 linear layer와 log-Softmax를 거쳐 단어를 예측하게 됩니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNyp6WP9PZNO",
        "colab_type": "code",
        "outputId": "49f63c5b-4ac9-41b6-be25-b2a00ddd00fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "!pip install --upgrade torchtext"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/ef/54b8da26f37787f5c670ae2199329e7dccf195c060b25628d99e587dac51/torchtext-0.5.0-py3-none-any.whl (73kB)\n",
            "\r\u001b[K     |████▌                           | 10kB 24.3MB/s eta 0:00:01\r\u001b[K     |█████████                       | 20kB 1.6MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 30kB 2.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 40kB 1.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 51kB 1.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 61kB 2.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 2.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.17.5)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.28.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.4.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 7.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "Successfully installed sentencepiece-0.1.85 torchtext-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avO861tYZ3ks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        \n",
        "        self.model_type = 'Transformer'\n",
        "        self.src_mask = None\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.ninp = ninp\n",
        "        self.decoder = nn.Linear(ninp, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src):\n",
        "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
        "            device = src.device\n",
        "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "            self.src_mask = mask\n",
        "\n",
        "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, self.src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZ5HdvhjdRKu",
        "colab_type": "text"
      },
      "source": [
        "`PositionalEncoding` 모듈은 sequence 내 토큰의 상대적 혹은 절대적 위치에 관한 정보를 집어넣습니다. Postional encoding은 embedding과 같은 차원을 갖아 이 둘을 더할 수 있습니다. 여기서 우리는 `sine`과 `cosine`을 사용하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJwBXg8edOoa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqnHf5cY8xuB",
        "colab_type": "text"
      },
      "source": [
        "> 저번과 마찬가지로 어렵습니다. 설명을 해놓을거면 똑바로 해놓던가... 튜토리얼 느낌이 나질 않네요. 한번 자세히 살펴봅시다.\n",
        "\n",
        "> ## Positional Encoding\n",
        "Positional Encoding 먼저 보겠습니다.앞서 설명했듯 transformer는 embedding layer를 통과한 후 positional encoding layer를 통과합니다. transformer는 RNN구조를 탈피했기 때문에, positional encoding을 통해 단어의 순서에 대한 정보를 주는 것입니다.  \n",
        "> <center>\n",
        "<img src=\"https://wikidocs.net/images/page/31379/transformer6_final.PNG\" height=70>\n",
        "</center>\n",
        "\n",
        "> positional encoding은 다음과 같은 함수를 이용해 진행됩니다.  \n",
        "$$\n",
        "PE_{(pos, 2i)} = \\sin (pos/10000^{2i/d_{model}})   \\\\\n",
        "PE_{(pos, 2i+1)} = \\cos (pos/10000^{2i/d_{model}})\n",
        "$$  \n",
        "\n",
        "여기서 pos는 embedding vector 내에서 위치를, i는 단어의 위치를 나타냅니다. 이를 문장의 개념에서 생각해보면 문장의 단어들의 embedding vector matrix와, positional encoding matrix를 더해주는 것으로 이해할 수 있습니다.\n",
        "<center>\n",
        "<img src=\"https://wikidocs.net/images/page/31379/transformer7.PNG\" height=150>\n",
        "</center>  \n",
        "\n",
        "> 코드를 보면 좀 더 명확합니다. 우선 encoding matrix와 positional encoding matrix를 더해줄 큰 matrix를 0으로 초기화합니다. 이를 `pe`에 할당합니다. 이후 position역할을 해줄 $pos$ sequence를 생성합니다.\n",
        "```python\n",
        "pe = torch.zeros(max_len, d_model)\n",
        "position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "```\n",
        "그리고 `div_term`에 $1/10000^{2i/d_{model}}$ sequence를 생성하고 `position`과 곱해 $PE$를 만들면 됩니다.\n",
        "```python\n",
        "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "pe[:, 0::2] = torch.sin(position * div_term)\n",
        "pe[:, 1::2] = torch.cos(position * div_term)\n",
        "```\n",
        "그러나 여기서 굳이 $\\exp(\\log(...))$의 형태로 한 것은 잘 이해가 가질 않네요. \n",
        "\n",
        "> positional encoding과 embedding이 결합된 matrix `pe`는 **[max len, d_model]**차원이 됩니다. 이를 **[1, max len, d_model]**로 만들고, transpose를 통해 **[max len, 1, d_model]**로 만듭니다. 일반적으로 RNN에서 사용하는 input모양과 닮았네요. 1은 batch size으로 보입니다.\n",
        "이후엔 `self.register_buffer`에 이 matrix를 할당해줍니다. `self.register_buffer`는 `parameter`는 아니기 때문에 grad에 영향을 받진 않지만, `state_dict`안에 저장됩니다. 또한 `model.parameters()`를 통해서 return받지도 않습니다.\n",
        "```python\n",
        "pe = pe.unsqueeze(0).transpose(0, 1)\n",
        " self.register_buffer = ('pe', pe)\n",
        "```\n",
        "\n",
        "> `forward`에서는 embedding matrix `x`와 `pe`를 더한 값을 return합니다. `pe`는 max_len에 대해 모든 값을 계산한 것이기 때문에, input의 length(`x.size(0)`)만큼만 뽑아서 더해줍니다.\n",
        "```python\n",
        "def forward(self, x):\n",
        "    x = x + self.pe[:x.size(0), :]\n",
        "    return self.dropout(x)\n",
        "```\n",
        "\n",
        "> 사실상 forward는 필요가 없고, 따라서 `nn.Module`을 상속받을 이유가 없습니다. 이는 parameter가 없어 gradient가 흐르지 않기 때문입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIa2fUpbZo1S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = PositionalEncoding(512)\n",
        "for param in model.parameters():\n",
        "    print(param)    # Empty!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqeeD5OmgqNE",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "> ## nn,TransformerEncoderLayer\n",
        "API에 따르면 `nn.TransformerEncoder`는 **N encoder layers의 stack**이라고 되어 있습니다. 따라서 `TransformerEncoderLayer`를 보겠습니다.  \n",
        "<center>\n",
        "<img src=\"https://i.imgur.com/G6XYFHl.jpg\" height=600>\n",
        "</center>  \n",
        "\n",
        "> `TransformerEncoderLayer`는 self-attn(MultiheadAttention)과 feedforward로 구성되어 있습니다. parameter는 다음과 같습니다.  \n",
        "- d_model: Encoder에서 input/output의 차원. Embedding vector의 크기도 d_model이 됨. 논문에선 512.\n",
        "- nhead: multiheadattention에서 head의 갯수로, 벡터를 nhead만큼 나누어 병렬로 attention을 진행.\n",
        "- dim_feedforward: transformer 내부 FF의 hidden 차원 (default=2048).\n",
        "- dropout: the dropout value (default=0.1).\n",
        "- activation: the activation function of intermediate layer, relu or gelu (default=relu).\n",
        "\n",
        "> `forward`의 Parameter는 다음과 같습니다.\n",
        "- src: Encoder에게 feed할 sequence\n",
        "- tgt: Decoder에게 feed할 sequence\n",
        "- src_mask: the additive mask for the src sequence (optional).\n",
        "- tgt_mask: the additive mask for the tgt sequence (optional).\n",
        "- memory_mask – the additive mask for the encoder output (optional).\n",
        "- src_key_padding_mask: the ByteTensor mask for src keys per batch (optional).\n",
        "- tgt_key_padding_mask: the ByteTensor mask for tgt keys per batch (optional).\n",
        "- memory_key_padding_mask: the ByteTensor mask for memory keys per batch (optional). \n",
        "\n",
        "> 생성자를 살펴보면 다음과 같이 되어 있습니다.\n",
        "\n",
        "```python\n",
        "def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear1 = Linear(d_model, dim_feedforward)\n",
        "        self.dropout = Dropout(dropout)\n",
        "        self.linear2 = Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = LayerNorm(d_model)\n",
        "        self.norm2 = LayerNorm(d_model)\n",
        "        self.dropout1 = Dropout(dropout)\n",
        "        self.dropout2 = Dropout(dropout)\n",
        "\n",
        "        self.activation = _get_activation_fn(activation)\n",
        "```  \n",
        "\n",
        "> 앞서 언급한 FFN과 seft-attention이 있는 것을 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81cDEhMsOYbp",
        "colab_type": "text"
      },
      "source": [
        "> ## `nn.MultiheadAttention`\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/max/395/1*4HJt3iD5tbtf9wZFuDrM-Q.png \" height=400>\n",
        "</center>\n",
        "\n",
        ">앞선 embedding + positional encoding의 결과는 위 그림의 빨간색 네모와 같이 세 개로 분리되어 multihead attention에 들어가게 됩니다. API 문서에서는 다음과 같이 input의 dimension을 설정해놨습니다.  \n",
        ">>Inputs:\n",
        "- query: ($L$, $N$, $E$) where L is the target sequence length, N is the batch size, E is the embedding dimension.\n",
        "- key: ($S$, $N$, $E$), where S is the source sequence length, N is the batch size, E is the embedding dimension.\n",
        "- value: ($S$, $N$, $E$) where S is the source sequence length, N is the batch size, E is the embedding dimension.\n",
        " \n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/max/305/1*4UvAIxzfkkUercHoGJuj3w.png \" height=300>\n",
        "</center>    \n",
        "\n",
        "> MultiheadAttention은 위와 같이 Scaled Dot-Product Attention 여러개로 구성되어 있습니다. 그림에서 보이는 h가 바로 앞서 보았던 `TransformerEncoder`의 Param. `nhead`를 의미합니다. $Q, K, V$는 모두 같은 것으로, input vector를 nhead로 나눈 것이 이들의 차원이됩니다. 원문에서는 d_model이 512이 이고, nhead(h)가 8이므로, $Q, K, V$의 차원은 $512/8=64$가 됩니다. PyTorch에서 이 코드를 찾아봤으나, `nn.functional.multi_head_attention_forward`을 통해 접근하고, 코드는 찾지 못하였습니다.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/max/533/1*szM6NDR-RhIip9IkwFTJMw.png\" height=300>\n",
        "</center>    \n",
        "\n",
        "> 그리고 Scaled Dot-Product Attention내에서 self attention이 일어나게 됩니다. 이를 수식으로 표현하면 다음과 같습니다.\n",
        " \n",
        "$$\n",
        "\\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n",
        "        \\text{where } head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
        "$$  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Wj4QYUM-Ul0",
        "colab_type": "text"
      },
      "source": [
        "# Load and batch data\n",
        "학습과정은 `torchtext`내의 Wikitext-2 dataset를 이용합니다. vocab 객체는 학습 데이터에 기반하여 만들어지고, token을 tensor로 numericalize하는데 이용됩니다. Sequential 데이터로부터 시작하여, `batchify()` 함수는 데이터셋을 column으로 정렬하여 데이터가 `batch_size` 크기의 배치로 나눠진 후 남은 토큰을 제거합니다. 예를 들어, 알파벳 sequence와 4의 batch size인 경우, 우리는 알파벳을 6의 길이의 4개의 sequence로 나타낼 것입니다.\n",
        "$$\n",
        "\\begin{split}\\begin{bmatrix}\n",
        "\\text{A} & \\text{B} & \\text{C} & \\ldots & \\text{X} & \\text{Y} & \\text{Z}\n",
        "\\end{bmatrix}\n",
        "\\Rightarrow\n",
        "\\begin{bmatrix}\n",
        "\\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &\n",
        "\\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &\n",
        "\\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &\n",
        "\\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n",
        "\\end{bmatrix}\\end{split}\n",
        "$$  \n",
        "이러한 컬럼은 모델에 의해 독립적으로 처리됩니다. 즉, `G`와 `F`의 관계는 학습될 수 없으나 더욱 효율적인 batch processing을 가능케합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jey31BQM_4SC",
        "colab_type": "code",
        "outputId": "e0ad1a4b-0995-4c0b-b157-56f44211dd9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "# Field 객체 생성\n",
        "TEXT  = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"),\n",
        "                             init_token='<sos>',\n",
        "                             eos_token='<eos>',\n",
        "                             lower=True)\n",
        "# splits 메소드로 데이터를 생성\n",
        "train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)\n",
        "# vocabulary 생성\n",
        "TEXT.build_vocab(train_txt)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def batchify(data, batch_size):\n",
        "    data = TEXT.numericalize([data.examples[0].text])\n",
        "    # dataset을 batch_size로 나눔\n",
        "    nbatch = data.size(0) // batch_size\n",
        "    # 나머지는 전부 잘라버림\n",
        "    data = data.narrow(0, 0, nbatch * batch_size)\n",
        "    # batch size batches로 균등하게 나눔\n",
        "    data = data.view(batch_size, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(train_txt, batch_size)\n",
        "val_data = batchify(val_txt, eval_batch_size)\n",
        "test_data = batchify(test_txt, eval_batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading wikitext-2-v1.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "wikitext-2-v1.zip: 100%|██████████| 4.48M/4.48M [00:01<00:00, 3.01MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "extracting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHAMstPSPq7z",
        "colab_type": "text"
      },
      "source": [
        "> 마찬가지로 자세히 살펴보겠습니다. 이전시간에서 보았듯, `Field`는 텍스트 데이터를 tensor로 변환하는 지시사항(instructions)과 datatype을 정의합니다.  \n",
        "```python\n",
        "TEXT  = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"),\n",
        "                             init_token='<sos>',\n",
        "                             eos_token='<eos>',\n",
        "                             lower=True)\n",
        "```  \n",
        "그 후엔 `torch.data.Dataset.splits`을 이용하여 train, test, validation으로 분리합니다. `splits` 메소드는 text_field를 인자로 받으며, 이는 text data에 쓰일 field를 의미합니다.\n",
        "```python\n",
        "train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)\n",
        "```\n",
        "이후엔 field 객체의 `build_vocab` 메소드를 통해 사전을 생성하면 끝입니다.\n",
        "```python\n",
        "TEXT.build_vocab(train_txt)\n",
        "```\n",
        "\n",
        "> `batchify()`는 앞서 언급했듯, 정해진 batch size로 데이터셋을 나누는 역할을 합니다. `Field`의 `numericalize`는 batch로 들어온 데이터를 field를 이용해 `Variable`로 바꾸는 역할을 합니다. `numericalize`는 **arr** *(List[List[str]], or tuple of (List[List[str]], List[int]))*을 인자로 받고, str은 tokenize되고 pad된 example입니다.\n",
        "```python\n",
        "data = TEXT.numericalize([data.examples[0].text])\n",
        "```\n",
        "그리고 nbatch를 계산한 후, `narrow`를 이용해 indexing하고, `contiguous`를 통해 데이터값을 가져옵니다. contiguous는 데이터의 idx를 가져오는 것이 아닌, 실제 데이터 값을 복사합니다. [이곳](https://hanseokhyeon.tistory.com/entry/PyTorch-contiguous-%ED%95%A8%EC%88%98)을 참조.\n",
        "```python\n",
        "nbatch = data.size(0) // batch_size\n",
        "data = data.narrow(dim=0, start=0, length=nbatch * batch_size) # data[0:nbatch * batch_size, :] 과 동일\n",
        "data = data.view(batch_size, -1).t().contiguous()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMGuVf3AoTfL",
        "colab_type": "text"
      },
      "source": [
        "# Functions to generate input and target sequence\n",
        "`get_batch()` 함수는 transformer model을 위한 input과 target sequence를 생성합니다. 이는 source data를 `bptt`길이의 뭉터기로 세분화합니다. Language model 작업을 위해서 모델은 뒤따라오는 다음 단어들을 `Target`으로 필요로 합니다. 예를 들어, `bptt`값이 2라고했을 때, `i`=0인 시점에서 다음 두 Variable을 얻을 것입니다.  \n",
        "![](https://pytorch.org/tutorials/_images/transformer_input_target.png)\n",
        "\n",
        "뭉터기로 자르는 작업은 dimension 0을 따라서 진행되고, Transformer 모델 내의 `S` 차원과 일치합니다. Batch 차원 N은 dimension 1을 따릅니다.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzDNK2sqPqe8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bptt = 35\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)   # cross entropy에 집어넣을 것이니 차원을 미리 조정\n",
        "    return data, target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1zot3aNrbPq",
        "colab_type": "text"
      },
      "source": [
        "# Initiate an instance\n",
        "모델은 아래의 hyperparameter를 따라 세팅됩니다. 사전의 크기는 vocab object의 길이와 같습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rx8qMmYf5Br",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ntokens = len(TEXT.vocab.stoi)\n",
        "emsize = 200\n",
        "nhid = 200\n",
        "nlayers = 2\n",
        "nhead = 2\n",
        "dropout = 0.2\n",
        "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unmm58MuszZo",
        "colab_type": "text"
      },
      "source": [
        "# Run the model\n",
        "CrossEntropyLoss를 통해 loss를 계산하고, SGD를 optimizer로 사용하겠습니다. 초기 lr은 0.5입니다. StepLR을 사용하여 epochs마다 lr을 조정하겠습니다. 학습과정동안 `nn.utils.clip_grad_norm_`를 사용하여 exploding을 방지하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5izabnfrOP7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0 # learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "import time\n",
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    start_time = time.time()\n",
        "    ntokens = len(TEXT.vocab.stoi)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        log_interval = 200  # \n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print(f'| epoch {epoch:3} | {batch:5}/{len(train_data) // bptt:5} batches | '\n",
        "                  f'lr {scheduler.get_lr()[0]:02.2} | ms/batch {elapsed * 1000 / log_interval:5.2} | '\n",
        "                  f'loss {cur_loss:5.2} | ppl {math.exp(cur_loss):8.2}')\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(eval_model, data_source):\n",
        "    eval_model.eval() # Turn on the evaluation mode\n",
        "    total_loss = 0.\n",
        "    ntokens = len(TEXT.vocab.stoi)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            output = eval_model(data)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgIsZ3F98XxG",
        "colab_type": "code",
        "outputId": "1d48b792-c684-4cab-c336-437c4ced7d5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 898
        }
      },
      "source": [
        "best_val_loss = float(\"inf\")\n",
        "epochs = 3 # The number of epochs\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train()\n",
        "    val_loss = evaluate(model, val_data)\n",
        "    print('-' * 89)\n",
        "    print(f'| end of epoch {epoch:3} | time: {(time.time() - epoch_start_time):5.2}s | valid loss {val_loss:5.2} | '\n",
        "          f'valid ppl {math.exp(val_loss):8.2}')\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = model\n",
        "\n",
        "    scheduler.step()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |   200/ 2981 batches | lr 5.0 | ms/batch 1.6e+01 | loss   8.0 | ppl  2.9e+03\n",
            "| epoch   1 |   400/ 2981 batches | lr 5.0 | ms/batch 1.5e+01 | loss   6.8 | ppl  8.9e+02\n",
            "| epoch   1 |   600/ 2981 batches | lr 5.0 | ms/batch 1.5e+01 | loss   6.4 | ppl  5.8e+02\n",
            "| epoch   1 |   800/ 2981 batches | lr 5.0 | ms/batch 1.5e+01 | loss   6.2 | ppl  5.1e+02\n",
            "| epoch   1 |  1000/ 2981 batches | lr 5.0 | ms/batch 1.5e+01 | loss   6.1 | ppl  4.5e+02\n",
            "| epoch   1 |  1200/ 2981 batches | lr 5.0 | ms/batch 1.5e+01 | loss   6.1 | ppl  4.4e+02\n",
            "| epoch   1 |  1400/ 2981 batches | lr 5.0 | ms/batch 1.5e+01 | loss   6.0 | ppl  4.2e+02\n",
            "| epoch   1 |  1600/ 2981 batches | lr 5.0 | ms/batch 1.5e+01 | loss   6.0 | ppl  4.2e+02\n",
            "| epoch   1 |  1800/ 2981 batches | lr 5.0 | ms/batch 1.4e+01 | loss   6.0 | ppl  3.9e+02\n",
            "| epoch   1 |  2000/ 2981 batches | lr 5.0 | ms/batch 1.4e+01 | loss   6.0 | ppl  3.9e+02\n",
            "| epoch   1 |  2200/ 2981 batches | lr 5.0 | ms/batch 1.5e+01 | loss   5.9 | ppl  3.5e+02\n",
            "| epoch   1 |  2400/ 2981 batches | lr 5.0 | ms/batch 1.5e+01 | loss   5.9 | ppl  3.6e+02\n",
            "| epoch   1 |  2600/ 2981 batches | lr 5.0 | ms/batch 1.5e+01 | loss   5.9 | ppl  3.6e+02\n",
            "| epoch   1 |  2800/ 2981 batches | lr 5.0 | ms/batch 1.4e+01 | loss   5.8 | ppl  3.3e+02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 4.6e+01s | valid loss   5.7 | valid ppl  3.1e+02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/ 2981 batches | lr 4.5 | ms/batch 1.5e+01 | loss   5.8 | ppl  3.3e+02\n",
            "| epoch   2 |   400/ 2981 batches | lr 4.5 | ms/batch 1.5e+01 | loss   5.8 | ppl  3.2e+02\n",
            "| epoch   2 |   600/ 2981 batches | lr 4.5 | ms/batch 1.5e+01 | loss   5.6 | ppl  2.7e+02\n",
            "| epoch   2 |   800/ 2981 batches | lr 4.5 | ms/batch 1.5e+01 | loss   5.6 | ppl  2.8e+02\n",
            "| epoch   2 |  1000/ 2981 batches | lr 4.5 | ms/batch 1.5e+01 | loss   5.6 | ppl  2.7e+02\n",
            "| epoch   2 |  1200/ 2981 batches | lr 4.5 | ms/batch 1.5e+01 | loss   5.6 | ppl  2.8e+02\n",
            "| epoch   2 |  1400/ 2981 batches | lr 4.5 | ms/batch 1.5e+01 | loss   5.6 | ppl  2.8e+02\n",
            "| epoch   2 |  1600/ 2981 batches | lr 4.5 | ms/batch 1.5e+01 | loss   5.7 | ppl  2.9e+02\n",
            "| epoch   2 |  1800/ 2981 batches | lr 4.5 | ms/batch 1.5e+01 | loss   5.6 | ppl  2.7e+02\n",
            "| epoch   2 |  2000/ 2981 batches | lr 4.5 | ms/batch 1.5e+01 | loss   5.6 | ppl  2.8e+02\n",
            "| epoch   2 |  2200/ 2981 batches | lr 4.5 | ms/batch 1.5e+01 | loss   5.5 | ppl  2.5e+02\n",
            "| epoch   2 |  2400/ 2981 batches | lr 4.5 | ms/batch 1.5e+01 | loss   5.6 | ppl  2.7e+02\n",
            "| epoch   2 |  2600/ 2981 batches | lr 4.5 | ms/batch 1.5e+01 | loss   5.6 | ppl  2.7e+02\n",
            "| epoch   2 |  2800/ 2981 batches | lr 4.5 | ms/batch 1.5e+01 | loss   5.5 | ppl  2.5e+02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 4.6e+01s | valid loss   5.6 | valid ppl  2.6e+02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 2981 batches | lr 4.3 | ms/batch 1.5e+01 | loss   5.6 | ppl  2.6e+02\n",
            "| epoch   3 |   400/ 2981 batches | lr 4.3 | ms/batch 1.5e+01 | loss   5.6 | ppl  2.6e+02\n",
            "| epoch   3 |   600/ 2981 batches | lr 4.3 | ms/batch 1.4e+01 | loss   5.4 | ppl  2.2e+02\n",
            "| epoch   3 |   800/ 2981 batches | lr 4.3 | ms/batch 1.5e+01 | loss   5.4 | ppl  2.3e+02\n",
            "| epoch   3 |  1000/ 2981 batches | lr 4.3 | ms/batch 1.5e+01 | loss   5.4 | ppl  2.2e+02\n",
            "| epoch   3 |  1200/ 2981 batches | lr 4.3 | ms/batch 1.5e+01 | loss   5.4 | ppl  2.3e+02\n",
            "| epoch   3 |  1400/ 2981 batches | lr 4.3 | ms/batch 1.5e+01 | loss   5.5 | ppl  2.4e+02\n",
            "| epoch   3 |  1600/ 2981 batches | lr 4.3 | ms/batch 1.5e+01 | loss   5.5 | ppl  2.4e+02\n",
            "| epoch   3 |  1800/ 2981 batches | lr 4.3 | ms/batch 1.5e+01 | loss   5.4 | ppl  2.2e+02\n",
            "| epoch   3 |  2000/ 2981 batches | lr 4.3 | ms/batch 1.5e+01 | loss   5.4 | ppl  2.3e+02\n",
            "| epoch   3 |  2200/ 2981 batches | lr 4.3 | ms/batch 1.5e+01 | loss   5.3 | ppl  2.1e+02\n",
            "| epoch   3 |  2400/ 2981 batches | lr 4.3 | ms/batch 1.5e+01 | loss   5.4 | ppl  2.2e+02\n",
            "| epoch   3 |  2600/ 2981 batches | lr 4.3 | ms/batch 1.5e+01 | loss   5.4 | ppl  2.3e+02\n",
            "| epoch   3 |  2800/ 2981 batches | lr 4.3 | ms/batch 1.5e+01 | loss   5.4 | ppl  2.1e+02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 4.6e+01s | valid loss   5.6 | valid ppl  2.7e+02\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhrgGswo9kq6",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate the model with the test dataset\n",
        "최고 성능을 보이는 모델을 사용해 test dataset을 확인해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FW2ONG779ZqY",
        "colab_type": "code",
        "outputId": "14b96318-ee5b-4ab5-f875-485e289933a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "test_loss = evaluate(best_model, test_data)\n",
        "print('=' * 89)\n",
        "print(f'| End of training | test loss {test_loss:5.2} | test ppl {math.exp(test_loss):8.2}')\n",
        "print('=' * 89)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss   5.5 | test ppl  2.4e+02\n",
            "=========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39oNPcvLTCfd",
        "colab_type": "text"
      },
      "source": [
        "> 이전까지는 기껏 Sampler니, collate_fn이니 Dataloader 잘도 알려줄 땐 언제고 이제와서 함수로 배치를 주는 이유를 모르겠습니다. 저는 공부를 하는 것이 목적이므로, 이를 한번 고쳐보겠습니다.  \n",
        "우선 저희가 할 작업을 다시 한번 살펴보겠습니다. 저희는 Language modeling을 하고 있고, 특정 word가 주어졌을 때, 다음 단어를 예측하는 것이 핵심입니다. 한번 데이터를 살펴보겠습니다. input은 **[bptt, batch_size]**입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vszg29SorBP2",
        "colab_type": "code",
        "outputId": "e7c5bf6b-a731-43d7-b2cb-948add99caca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "### 원문대로 했을 경우\n",
        "print(TEXT.numericalize([train_txt.examples[0].text]).size())   # [2086708, 1]\n",
        "# batch size (20) 단위로 잘라주고, 나머지는 drop out \n",
        "print(train_data.size())    # [104335, 20]\n",
        "for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "    data, targets = get_batch(train_data, i)\n",
        "    print(data.size())    # [35, 20] == [bptt, batch_size]\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2086708, 1])\n",
            "torch.Size([104335, 20])\n",
            "torch.Size([35, 20])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnsMM_npw7Fh",
        "colab_type": "text"
      },
      "source": [
        "> 원본은 **[2086708, 1]**를 **[20, 104335]**로 만든 후, transpose하여 잘라내 학습시켰습니다. 다음과 같은 과정을 거쳐도 같은 데이터임을 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIbSa49xxpxn",
        "colab_type": "code",
        "outputId": "ea5ca870-604f-482f-8e4d-b8514e9956e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "data = TEXT.numericalize([train_txt.examples[0].text])\n",
        "by = len(train_txt.examples[0].text) % batch_size\n",
        "print(by)\n",
        "data = data[:-by, :]\n",
        "print(data.size())\n",
        "data.view(20, -1).t().to(device) == train_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n",
            "torch.Size([2086700, 1])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        ...,\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRvzpaaf99f6",
        "colab_type": "code",
        "outputId": "0f3088e3-0fca-4cc9-be9d-0fc3e72f8fd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "# Fields.vocab.itos를 통해 결과 출력\n",
        "for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "    data, targets = get_batch(train_data, i)\n",
        "    for datum, target in zip(data.t()[:3], targets.view(35, 20).t()[:3]):\n",
        "        print(\"input: \")\n",
        "        print(\" \"*2, \" \".join(TEXT.vocab.itos[item] for item in datum))\n",
        "        print(\"output: \")\n",
        "        print(\" \"*2, \" \".join(TEXT.vocab.itos[item] for item in target))\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input: \n",
            "   <eos> = valkyria chronicles iii = <eos> <eos> senjō no valkyria 3 <unk> chronicles ( japanese 戦場のヴァルキュリア3 , lit . valkyria of the battlefield 3 ) , commonly referred to as valkyria chronicles iii outside\n",
            "output: \n",
            "   = valkyria chronicles iii = <eos> <eos> senjō no valkyria 3 <unk> chronicles ( japanese 戦場のヴァルキュリア3 , lit . valkyria of the battlefield 3 ) , commonly referred to as valkyria chronicles iii outside japan\n",
            "input: \n",
            "   @ 1 rebounds , 3 @ . @ 8 assists , and 1 @ . @ 5 steals per game . he also shot 45 % from the field , and 82 % from the\n",
            "output: \n",
            "   1 rebounds , 3 @ . @ 8 assists , and 1 @ . @ 5 steals per game . he also shot 45 % from the field , and 82 % from the free\n",
            "input: \n",
            "   settlement was rapid in particular , there was an influx of german families from wisconsin . the town of madison was officially <unk> by barnes in 1870 or 1871 . in 1875 , it became\n",
            "output: \n",
            "   was rapid in particular , there was an influx of german families from wisconsin . the town of madison was officially <unk> by barnes in 1870 or 1871 . in 1875 , it became the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFEX0w1eF3pD",
        "colab_type": "code",
        "outputId": "d19cf497-0759-44a5-c3f4-695f9957e459",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# 원본 데이터와 비교\n",
        "\" \".join([TEXT.vocab.itos[item] for item in TEXT.numericalize([train_txt.examples[0].text])[:35*20*3]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<eos> = valkyria chronicles iii = <eos> <eos> senjō no valkyria 3 <unk> chronicles ( japanese 戦場のヴァルキュリア3 , lit . valkyria of the battlefield 3 ) , commonly referred to as valkyria chronicles iii outside japan , is a tactical role @-@ playing video game developed by sega and media . vision for the playstation portable . released in january 2011 in japan , it is the third game in the valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the nameless , a penal military unit serving the nation of gallia during the second europan war who perform secret black operations and are pitted against the imperial unit <unk> raven . <eos> the game began development in 2010 , carrying over a large portion of the work done on valkyria chronicles ii . while it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more <unk> for series newcomers . character designer <unk> honjou and composer hitoshi sakimoto both returned from previous entries , along with valkyria chronicles ii director takeshi ozawa . a large team of writers handled the script . the game ' s opening theme was sung by may ' n . <eos> it met with positive sales in japan , and was praised by both japanese and western critics . after release , it received downloadable content , along with an expanded edition in november of that year . it was also adapted into manga and an original video animation series . due to low sales of valkyria chronicles ii , valkyria chronicles iii was not localized , but a fan translation compatible with the game ' s expanded edition was released in 2014 . media . vision would return to the franchise with the development of valkyria azure revolution for the playstation 4 . <eos> <eos> = = gameplay = = <eos> <eos> as with previous <unk> chronicles games , valkyria chronicles iii is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces . stories are told through comic book @-@ like panels with animated character portraits , with characters speaking partially through voiced speech bubbles and partially through <unk> text . the player progresses through a series of linear missions , gradually unlocked as maps that can be freely <unk> through and replayed as they are unlocked . the route to each story location on the map varies depending on an individual player ' s approach when one option is selected , the other is sealed off to the player . outside missions , the player characters rest in a camp , where units can be customized and character growth occurs . alongside the main story missions are character @-@ specific sub missions relating to different squad members . after the game ' s completion , additional episodes are unlocked , some of them having a higher difficulty than those found in the rest of the game . there are also love simulation elements related to the game ' s two main <unk> , although they take a very minor role . <eos> the game ' s battle system , the <unk> system , is carried over directly from <unk> chronicles . during missions , players select each unit using a top @-@ down perspective of the battlefield map once a character is selected , the player moves the character around the battlefield in third @-@ person . a character can only act once per @-@ turn , but characters can be granted multiple turns at the expense of other characters ' turns . each character has a field and distance of movement limited by their action <unk> . up to nine characters can be assigned to a single mission . during gameplay , characters will call out if something happens to them , such as their health points ( hp ) getting low or being knocked out by enemy attacks . each character has specific potentials , skills unique to each character . they are divided into personal potential , which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character , and battle potentials , which are grown throughout the game and always grant <unk> to a character . to learn battle potentials , each character has a unique masters table , a grid @-@ based skill table that can be used to acquire and link different skills . characters also have special <unk> that grant them temporary <unk> on the battlefield kurt can activate direct command and move around the battlefield without <unk> his action point gauge , the character <unk> can shift into her valkyria form and become <unk> , while imca can target multiple enemy units with her heavy weapon . <eos> troops are divided into five classes scouts , <unk> , engineers , <unk> and armored soldier . <unk> can switch classes by changing their assigned weapon . changing class does not greatly affect the stats gained while in a previous class . with victory in battle , experience points are awarded to the squad , which are distributed into five different attributes shared by the entire squad , a feature differing from early games ' method of distributing to different unit types . <eos> <eos> = = plot = = <eos> <eos> the game takes place during the second europan war . gallian army squad 422 , also known as the nameless , are a penal military unit composed of criminals , foreign <unk> , and military offenders whose real names are erased from the records and <unk> officially referred to by numbers . <unk> by the gallian military to perform the most dangerous missions that the regular army and militia will not do , they are nevertheless up to the task , exemplified by their motto , <unk> <unk> , meaning always ready . the three main characters are <unk> kurt irving , an army officer falsely accused of treason who wishes to redeem himself ace <unk> imca , a female darcsen heavy weapons specialist who seeks revenge against the valkyria who destroyed her home and <unk> riela <unk> , a seemingly <unk> young woman who is unknowingly a descendant of the valkyria . together with their fellow squad members , these three are tasked to fight against a mysterious imperial unit known as calamity raven , consisting of mostly darcsen soldiers . <eos> as the nameless officially do not exist , the upper echelons of the gallian army exploit the concept of plausible <unk> in order to send them on missions that would otherwise make gallia lose face in the war . while at times this works to their advantage , such as a successful incursion into imperial territory , other orders cause certain members of the 422nd great distress . one such member , <unk> , becomes so enraged that he abandons his post and defects into the ranks of calamity raven , attached to the ideal of darcsen independence proposed by their leader , dahau . at the same time , elements within gallian army command move to erase the nameless in order to protect their own interests . <unk> by both allies and enemies , and combined with the presence of a traitor within their ranks , the 422nd desperately move to keep themselves alive while at the same time fight to help the gallian war effort . this continues until the nameless ' s commanding officer , ramsey crowe , who had been kept under house arrest , is escorted to the capital city of <unk> in order to present evidence <unk> the weary soldiers and expose the real traitor , the gallian general that had accused kurt of treason . <eos> <unk> due to these events , and partly due to the major losses in manpower gallia suffers towards the end of the war with the empire , the nameless are offered a formal position as a squad in the gallian army rather than serve as an anonymous shadow force . this is short @-@ lived , however , as following maximilian ' s defeat , dahau and calamity raven move to activate an ancient <unk> super weapon within the empire , kept secret by their benefactor . without the support of maximilian or the chance to prove themselves in the war with gallia , it is dahau ' s last <unk> card in creating a new darcsen nation . as an armed gallian force invading the empire just following the two nations ' cease @-@ fire would certainly wreck their newfound peace , kurt decides to once again make his squad the nameless , asking crowe to list himself and all under his command as killed @-@ in @-@ action . now owing allegiance to none other than themselves , the 422nd confronts dahau and destroys the <unk> weapon . each member then goes their separate ways in order to begin their lives <unk> . <eos> <eos> = = development = = <eos> <eos> concept work for valkyria chronicles iii began after development finished on valkyria chronicles ii in early 2010 , with full development beginning shortly after this . the director of valkyria chronicles ii , takeshi ozawa , returned to that role for valkyria chronicles iii . development work took approximately one year . after the release of valkyria chronicles ii , the staff took a look at both the popular response for the game and what they wanted to do next for the series . like its predecessor , valkyria chronicles iii was developed for playstation portable this was due to the team wanting to refine the mechanics created for valkyria chronicles ii , and they had not come up with the revolutionary idea that would warrant a new entry for the playstation 3 . speaking in an interview , it was stated that the development team considered valkyria chronicles iii to be the series ' first true sequel while valkyria chronicles ii had required a large amount of trial and error during development due to the platform move , the third game gave them a chance to improve upon the best parts of valkyria chronicles ii due to being on the same platform . in addition to sega staff from the previous games , development work was also handled by <unk> the original scenario was written <unk> <unk> , while the script was written by hiroyuki <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> and <unk> <unk> . its story was darker and more somber than that of its predecessor . <eos> the majority of material created for previous games , such as the <unk> system and the design of maps , was carried over . alongside this , improvements were made to the game ' s graphics and some elements were expanded , such as map layouts , mission structure , and the number of playable units per mission . a part of this upgrade involved creating unique <unk> models for each character ' s body . in order to achieve this , the cooperative elements incorporated into the second game were removed , as they took up a large portion of memory space needed for the improvements . they also adjusted the difficulty settings and ease of play so they could appeal to new players while retaining the essential components of the series ' gameplay . the newer systems were decided upon early in development . the character designs were done by <unk> honjou , who had worked on the previous valkyria chronicles games . when creating the nameless squad , honjou was faced with the same problem he had had during the first game the military uniforms essentially destroyed character individuality , despite him needing to create unique characters the player could identify while maintaining a sense of reality within the valkyria chronicles world . the main color of the nameless was black . as with the previous valkyria games , valkyria chronicles iii used the <unk> graphics engine . the anime opening was produced by production i . g . <eos> <eos> = = = music = = = <eos> <eos> the music was composed by hitoshi sakimoto , who had also worked on the previous valkyria chronicles games . when he originally heard about the\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MLqZdeFpSQp",
        "colab_type": "text"
      },
      "source": [
        "> 함수를 직접 만들어서 쓴 것이 아니꼬우니 바꿔봅시다. 우선 `collate_fn`에 들어갈 함수를 만들겠습니다.\n",
        "\n",
        "```python\n",
        "def generate_batch(data):\n",
        "    # [1, 20]의 list가 들어옴.\n",
        "    print(len(data), data[0].size())\n",
        "    data = torch.cat([item for item in data])\n",
        "    data = data.view(batch_size, -1).t()\n",
        "    return data\n",
        "```\n",
        "\n",
        "> 이 함수는 위와 동일한 기능을 하지만, 실제 나오는 데이터는 다릅니다. 이는 아까 보았던 **[2086708, 1]** tensor를 먼저 $batch size \\times bptt$로 자른 후 **[bptt, batch size]**로 만든 것입니다. 그러나 저희 데이터는 sequential 하여, 어떠한 식으로 잘라내도 학습이 잘 되므로 상관없습니다. \n",
        "\n",
        "```python\n",
        "...\n",
        "data = TEXT.numericalize([train_txt.examples[0].text])\n",
        "data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size * bptt, collate_fn=generate_batch)\n",
        "for batch, i in enumerate(data_loader):\n",
        "    if batch == 0:\n",
        "        input_ = i.to(device)\n",
        "        pass\n",
        "    optimizer.zero_grad()\n",
        "    output = model(input_.to(device))\n",
        "    output = output.view(-1, ntokens)\n",
        "    targets = i.reshape(-1)\n",
        "...\n",
        "```\n",
        "> `DataLoader`를 이용하려 했으나 약간 애매합니다. 저희는 $t$시점의 데이터를 학습하고, $t+1$시점의 데이터를 예측합니다. 따라서 index로 접근하는 것이 더욱 효율적입니다. `DataLoader`를 사용하려 했더니 이게 불가능하여 0번째 시점에선 학습을 하지 않고 그 다음 시점에서 학습을 하는 것을 바꾸었습니다. 그러나 이 역시 문제가 발생했는데, 마지막에선 drop을 하지 않았기에 차원이 맞지 않는 문제가 발생합니다. 즉, 마지막 시점에서 인풋은 batch size만큼 있는데, target은 8개 밖에 없어 문제가 생깁니다.\n",
        "```python\n",
        "ValueError: Expected input batch_size (64) to match target batch_size (700).\n",
        "```\n",
        "물론 `DataLoader`내에서 `drop_last=True`를 주면 되긴 합니다만, 갑자기 `drop_out=True`가 하기 싫어졌습니다. 다른 방법을 찾아보았는데, `torch.utils.data.SequentialSampler`가 있었습니다. 한번 이 것을 이용해보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68z5w10kNCTA",
        "colab_type": "text"
      },
      "source": [
        "> 우선 저희가 필요한 것은 idx를 가져오는 `Sampler`입니다. 그리고 데이터셋을 정의하고, `DataLoader`내의 `sampler=`인자에 집어넣으면 됩니다.  \n",
        "`Dataset`은 input과 target을 내놓습니다. 이 둘은 t, t+1 시점으로 각 각 index됩니다. 그 후 `SequentialSampler`를 통해 sampler로 만들고, DataLoader로 뽑습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEvo_ugwzDNt",
        "colab_type": "code",
        "outputId": "fe9ed47c-6e9d-4470-d0b3-9956fb06c21b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        }
      },
      "source": [
        "class LMDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dset, bptt):\n",
        "        drop_idx = dset.size(0) % bptt\n",
        "        dset = dset[:-drop_idx, :]\n",
        "        self.dset = dset.view(-1, bptt)\n",
        "        self.bptt = bptt\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return [self.dset[idx, :], self.dset[idx+1, :]]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dset.size(0) - batch_size - 1\n",
        "\n",
        "data = TEXT.numericalize([train_txt.examples[0].text])\n",
        "dset = LMDataset(data, 35)\n",
        "sampler = torch.utils.data.SequentialSampler(dset)\n",
        "for i, (input_, targets) in enumerate(torch.utils.data.DataLoader(dset, batch_size=20, sampler=sampler)):\n",
        "    print(i, input_.t().size())\n",
        "    for datum, target in zip(input_.t()[:3], targets.t()[:3]):\n",
        "        print(\"input: \")\n",
        "        print(\" \"*2, \" \".join(TEXT.vocab.itos[item] for item in datum))\n",
        "        print(\"output: \")\n",
        "        print(\" \"*2, \" \".join(TEXT.vocab.itos[item] for item in target))\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 torch.Size([35, 20])\n",
            "input: \n",
            "   <eos> japan the unit over <unk> script release of the tactical , and off sub game <unk> player ' call\n",
            "output: \n",
            "   japan the unit over <unk> script release of the tactical , and off sub game <unk> player ' call each\n",
            "input: \n",
            "   = , valkyria serving a for . , valkyria franchise role with replayed to missions . system moves turns out\n",
            "output: \n",
            "   , valkyria serving a for . , valkyria franchise role with replayed to missions . system moves turns out character\n",
            "input: \n",
            "   valkyria is series the large series the it chronicles with @-@ characters as the relating there , the . if\n",
            "output: \n",
            "   is series the large series the it chronicles with @-@ characters as the relating there , the . if .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lfbYaUuAJZq",
        "colab_type": "text"
      },
      "source": [
        "> 이번엔 bptt없이 **[700, 1]** 짜리로 데이터를 받는 방법임. 이 경우 seq_len(bptt)가 700이고, batch_size는 1이 됨."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONg82YRah296",
        "colab_type": "code",
        "outputId": "b868731d-67fe-434c-93b4-86d3dfb56cb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "source": [
        "# batch 없이 해보는거\n",
        "# sampler도 지워봄\n",
        "class LMDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dset):\n",
        "        self.dset = dset\n",
        "        self.bptt = bptt\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return [self.dset[idx], self.dset[idx+1]]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dset.size(0) - batch_size - 1\n",
        "\n",
        "data = TEXT.numericalize([train_txt.examples[0].text])\n",
        "dset = LMDataset(data)\n",
        "for i, (input_, targets) in enumerate(torch.utils.data.DataLoader(dset, batch_size=700)):\n",
        "    print(i, input_.size())\n",
        "    for datum, target in zip(input_.t()[:3], targets.t()[:3]):\n",
        "        print(\"input: \")\n",
        "        print(\" \"*2, \" \".join(TEXT.vocab.itos[item] for item in datum))\n",
        "        print(\"output: \")\n",
        "        print(\" \"*2, \" \".join(TEXT.vocab.itos[item] for item in target))\n",
        "    if i == 1:\n",
        "        break\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 torch.Size([700, 1])\n",
            "input: \n",
            "   <eos> = valkyria chronicles iii = <eos> <eos> senjō no valkyria 3 <unk> chronicles ( japanese 戦場のヴァルキュリア3 , lit . valkyria of the battlefield 3 ) , commonly referred to as valkyria chronicles iii outside japan , is a tactical role @-@ playing video game developed by sega and media . vision for the playstation portable . released in january 2011 in japan , it is the third game in the valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the nameless , a penal military unit serving the nation of gallia during the second europan war who perform secret black operations and are pitted against the imperial unit <unk> raven . <eos> the game began development in 2010 , carrying over a large portion of the work done on valkyria chronicles ii . while it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more <unk> for series newcomers . character designer <unk> honjou and composer hitoshi sakimoto both returned from previous entries , along with valkyria chronicles ii director takeshi ozawa . a large team of writers handled the script . the game ' s opening theme was sung by may ' n . <eos> it met with positive sales in japan , and was praised by both japanese and western critics . after release , it received downloadable content , along with an expanded edition in november of that year . it was also adapted into manga and an original video animation series . due to low sales of valkyria chronicles ii , valkyria chronicles iii was not localized , but a fan translation compatible with the game ' s expanded edition was released in 2014 . media . vision would return to the franchise with the development of valkyria azure revolution for the playstation 4 . <eos> <eos> = = gameplay = = <eos> <eos> as with previous <unk> chronicles games , valkyria chronicles iii is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces . stories are told through comic book @-@ like panels with animated character portraits , with characters speaking partially through voiced speech bubbles and partially through <unk> text . the player progresses through a series of linear missions , gradually unlocked as maps that can be freely <unk> through and replayed as they are unlocked . the route to each story location on the map varies depending on an individual player ' s approach when one option is selected , the other is sealed off to the player . outside missions , the player characters rest in a camp , where units can be customized and character growth occurs . alongside the main story missions are character @-@ specific sub missions relating to different squad members . after the game ' s completion , additional episodes are unlocked , some of them having a higher difficulty than those found in the rest of the game . there are also love simulation elements related to the game ' s two main <unk> , although they take a very minor role . <eos> the game ' s battle system , the <unk> system , is carried over directly from <unk> chronicles . during missions , players select each unit using a top @-@ down perspective of the battlefield map once a character is selected , the player moves the character around the battlefield in third @-@ person . a character can only act once per @-@ turn , but characters can be granted multiple turns at the expense of other characters ' turns . each character has a field and distance of movement limited by their action <unk> . up to nine characters can be assigned to a single mission . during gameplay , characters will call out if something happens to them , such as their health points ( hp ) getting low or being knocked out by enemy attacks . each character has specific potentials , skills unique to\n",
            "output: \n",
            "   = valkyria chronicles iii = <eos> <eos> senjō no valkyria 3 <unk> chronicles ( japanese 戦場のヴァルキュリア3 , lit . valkyria of the battlefield 3 ) , commonly referred to as valkyria chronicles iii outside japan , is a tactical role @-@ playing video game developed by sega and media . vision for the playstation portable . released in january 2011 in japan , it is the third game in the valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the nameless , a penal military unit serving the nation of gallia during the second europan war who perform secret black operations and are pitted against the imperial unit <unk> raven . <eos> the game began development in 2010 , carrying over a large portion of the work done on valkyria chronicles ii . while it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more <unk> for series newcomers . character designer <unk> honjou and composer hitoshi sakimoto both returned from previous entries , along with valkyria chronicles ii director takeshi ozawa . a large team of writers handled the script . the game ' s opening theme was sung by may ' n . <eos> it met with positive sales in japan , and was praised by both japanese and western critics . after release , it received downloadable content , along with an expanded edition in november of that year . it was also adapted into manga and an original video animation series . due to low sales of valkyria chronicles ii , valkyria chronicles iii was not localized , but a fan translation compatible with the game ' s expanded edition was released in 2014 . media . vision would return to the franchise with the development of valkyria azure revolution for the playstation 4 . <eos> <eos> = = gameplay = = <eos> <eos> as with previous <unk> chronicles games , valkyria chronicles iii is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces . stories are told through comic book @-@ like panels with animated character portraits , with characters speaking partially through voiced speech bubbles and partially through <unk> text . the player progresses through a series of linear missions , gradually unlocked as maps that can be freely <unk> through and replayed as they are unlocked . the route to each story location on the map varies depending on an individual player ' s approach when one option is selected , the other is sealed off to the player . outside missions , the player characters rest in a camp , where units can be customized and character growth occurs . alongside the main story missions are character @-@ specific sub missions relating to different squad members . after the game ' s completion , additional episodes are unlocked , some of them having a higher difficulty than those found in the rest of the game . there are also love simulation elements related to the game ' s two main <unk> , although they take a very minor role . <eos> the game ' s battle system , the <unk> system , is carried over directly from <unk> chronicles . during missions , players select each unit using a top @-@ down perspective of the battlefield map once a character is selected , the player moves the character around the battlefield in third @-@ person . a character can only act once per @-@ turn , but characters can be granted multiple turns at the expense of other characters ' turns . each character has a field and distance of movement limited by their action <unk> . up to nine characters can be assigned to a single mission . during gameplay , characters will call out if something happens to them , such as their health points ( hp ) getting low or being knocked out by enemy attacks . each character has specific potentials , skills unique to each\n",
            "1 torch.Size([700, 1])\n",
            "input: \n",
            "   each character . they are divided into personal potential , which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character , and battle potentials , which are grown throughout the game and always grant <unk> to a character . to learn battle potentials , each character has a unique masters table , a grid @-@ based skill table that can be used to acquire and link different skills . characters also have special <unk> that grant them temporary <unk> on the battlefield kurt can activate direct command and move around the battlefield without <unk> his action point gauge , the character <unk> can shift into her valkyria form and become <unk> , while imca can target multiple enemy units with her heavy weapon . <eos> troops are divided into five classes scouts , <unk> , engineers , <unk> and armored soldier . <unk> can switch classes by changing their assigned weapon . changing class does not greatly affect the stats gained while in a previous class . with victory in battle , experience points are awarded to the squad , which are distributed into five different attributes shared by the entire squad , a feature differing from early games ' method of distributing to different unit types . <eos> <eos> = = plot = = <eos> <eos> the game takes place during the second europan war . gallian army squad 422 , also known as the nameless , are a penal military unit composed of criminals , foreign <unk> , and military offenders whose real names are erased from the records and <unk> officially referred to by numbers . <unk> by the gallian military to perform the most dangerous missions that the regular army and militia will not do , they are nevertheless up to the task , exemplified by their motto , <unk> <unk> , meaning always ready . the three main characters are <unk> kurt irving , an army officer falsely accused of treason who wishes to redeem himself ace <unk> imca , a female darcsen heavy weapons specialist who seeks revenge against the valkyria who destroyed her home and <unk> riela <unk> , a seemingly <unk> young woman who is unknowingly a descendant of the valkyria . together with their fellow squad members , these three are tasked to fight against a mysterious imperial unit known as calamity raven , consisting of mostly darcsen soldiers . <eos> as the nameless officially do not exist , the upper echelons of the gallian army exploit the concept of plausible <unk> in order to send them on missions that would otherwise make gallia lose face in the war . while at times this works to their advantage , such as a successful incursion into imperial territory , other orders cause certain members of the 422nd great distress . one such member , <unk> , becomes so enraged that he abandons his post and defects into the ranks of calamity raven , attached to the ideal of darcsen independence proposed by their leader , dahau . at the same time , elements within gallian army command move to erase the nameless in order to protect their own interests . <unk> by both allies and enemies , and combined with the presence of a traitor within their ranks , the 422nd desperately move to keep themselves alive while at the same time fight to help the gallian war effort . this continues until the nameless ' s commanding officer , ramsey crowe , who had been kept under house arrest , is escorted to the capital city of <unk> in order to present evidence <unk> the weary soldiers and expose the real traitor , the gallian general that had accused kurt of treason . <eos> <unk> due to these events , and partly due to the major losses in manpower gallia suffers towards the end of the war with the empire , the nameless are offered a formal position as a squad in the gallian army rather than serve as an anonymous shadow force . this is short @-@ lived , however , as following maximilian ' s defeat , dahau\n",
            "output: \n",
            "   character . they are divided into personal potential , which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character , and battle potentials , which are grown throughout the game and always grant <unk> to a character . to learn battle potentials , each character has a unique masters table , a grid @-@ based skill table that can be used to acquire and link different skills . characters also have special <unk> that grant them temporary <unk> on the battlefield kurt can activate direct command and move around the battlefield without <unk> his action point gauge , the character <unk> can shift into her valkyria form and become <unk> , while imca can target multiple enemy units with her heavy weapon . <eos> troops are divided into five classes scouts , <unk> , engineers , <unk> and armored soldier . <unk> can switch classes by changing their assigned weapon . changing class does not greatly affect the stats gained while in a previous class . with victory in battle , experience points are awarded to the squad , which are distributed into five different attributes shared by the entire squad , a feature differing from early games ' method of distributing to different unit types . <eos> <eos> = = plot = = <eos> <eos> the game takes place during the second europan war . gallian army squad 422 , also known as the nameless , are a penal military unit composed of criminals , foreign <unk> , and military offenders whose real names are erased from the records and <unk> officially referred to by numbers . <unk> by the gallian military to perform the most dangerous missions that the regular army and militia will not do , they are nevertheless up to the task , exemplified by their motto , <unk> <unk> , meaning always ready . the three main characters are <unk> kurt irving , an army officer falsely accused of treason who wishes to redeem himself ace <unk> imca , a female darcsen heavy weapons specialist who seeks revenge against the valkyria who destroyed her home and <unk> riela <unk> , a seemingly <unk> young woman who is unknowingly a descendant of the valkyria . together with their fellow squad members , these three are tasked to fight against a mysterious imperial unit known as calamity raven , consisting of mostly darcsen soldiers . <eos> as the nameless officially do not exist , the upper echelons of the gallian army exploit the concept of plausible <unk> in order to send them on missions that would otherwise make gallia lose face in the war . while at times this works to their advantage , such as a successful incursion into imperial territory , other orders cause certain members of the 422nd great distress . one such member , <unk> , becomes so enraged that he abandons his post and defects into the ranks of calamity raven , attached to the ideal of darcsen independence proposed by their leader , dahau . at the same time , elements within gallian army command move to erase the nameless in order to protect their own interests . <unk> by both allies and enemies , and combined with the presence of a traitor within their ranks , the 422nd desperately move to keep themselves alive while at the same time fight to help the gallian war effort . this continues until the nameless ' s commanding officer , ramsey crowe , who had been kept under house arrest , is escorted to the capital city of <unk> in order to present evidence <unk> the weary soldiers and expose the real traitor , the gallian general that had accused kurt of treason . <eos> <unk> due to these events , and partly due to the major losses in manpower gallia suffers towards the end of the war with the empire , the nameless are offered a formal position as a squad in the gallian army rather than serve as an anonymous shadow force . this is short @-@ lived , however , as following maximilian ' s defeat , dahau and\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIvPz3x68unr",
        "colab_type": "code",
        "outputId": "ef244717-73ad-456d-c45d-fd8061a5d726",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 898
        }
      },
      "source": [
        "ntokens = len(TEXT.vocab.stoi)\n",
        "emsize = 200\n",
        "nhid = 200\n",
        "nlayers = 2\n",
        "nhead = 2\n",
        "dropout = 0.2\n",
        "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0 # learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "import time\n",
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    start_time = time.time()\n",
        "    ntokens = len(TEXT.vocab.stoi)\n",
        "    data = TEXT.numericalize([train_txt.examples[0].text])\n",
        "    dset = LMDataset(data)\n",
        "    for batch, (data, targets) in enumerate(torch.utils.data.DataLoader(dset, batch_size=700)):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data.to(device))\n",
        "        loss = criterion(output.view(-1, ntokens), targets.squeeze(1).to(device))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        log_interval = 200  \n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print(f'| epoch {epoch:3} | {batch:5}/{len(dset) // 700:5} batches | '\n",
        "                  f'lr {scheduler.get_lr()[0]:02.2} | ms/batch {elapsed * 1000 / log_interval:5.2} | '\n",
        "                  f'loss {cur_loss:5.2} | ppl {math.exp(cur_loss):8.2}')\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(eval_model, data_source):\n",
        "    eval_model.eval() # Turn on the evaluation mode\n",
        "    total_loss = 0.\n",
        "    ntokens = len(TEXT.vocab.stoi)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            output = eval_model(data)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "epochs = 3 # The number of epochs\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train()\n",
        "    val_loss = evaluate(model, val_data)\n",
        "    print('-' * 89)\n",
        "    print(f'| end of epoch {epoch:3} | time: {(time.time() - epoch_start_time):5.2}s | valid loss {val_loss:5.2} | '\n",
        "          f'valid ppl {math.exp(val_loss):8.2}')\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = model\n",
        "\n",
        "    scheduler.step()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |   200/ 2980 batches | lr 5.0 | ms/batch 4.8e+01 | loss   8.0 | ppl  2.9e+03\n",
            "| epoch   1 |   400/ 2980 batches | lr 5.0 | ms/batch 4.6e+01 | loss   6.8 | ppl  9.2e+02\n",
            "| epoch   1 |   600/ 2980 batches | lr 5.0 | ms/batch 4.6e+01 | loss   6.6 | ppl  7.4e+02\n",
            "| epoch   1 |   800/ 2980 batches | lr 5.0 | ms/batch 4.6e+01 | loss   6.4 | ppl  5.8e+02\n",
            "| epoch   1 |  1000/ 2980 batches | lr 5.0 | ms/batch 4.5e+01 | loss   6.2 | ppl  5.1e+02\n",
            "| epoch   1 |  1200/ 2980 batches | lr 5.0 | ms/batch 4.6e+01 | loss   6.2 | ppl  4.8e+02\n",
            "| epoch   1 |  1400/ 2980 batches | lr 5.0 | ms/batch 4.5e+01 | loss   6.1 | ppl  4.3e+02\n",
            "| epoch   1 |  1600/ 2980 batches | lr 5.0 | ms/batch 4.6e+01 | loss   6.0 | ppl  4.2e+02\n",
            "| epoch   1 |  1800/ 2980 batches | lr 5.0 | ms/batch 4.5e+01 | loss   6.0 | ppl  4.1e+02\n",
            "| epoch   1 |  2000/ 2980 batches | lr 5.0 | ms/batch 4.6e+01 | loss   6.0 | ppl  4.1e+02\n",
            "| epoch   1 |  2200/ 2980 batches | lr 5.0 | ms/batch 4.5e+01 | loss   6.0 | ppl  3.9e+02\n",
            "| epoch   1 |  2400/ 2980 batches | lr 5.0 | ms/batch 4.6e+01 | loss   6.0 | ppl    4e+02\n",
            "| epoch   1 |  2600/ 2980 batches | lr 5.0 | ms/batch 4.5e+01 | loss   6.0 | ppl    4e+02\n",
            "| epoch   1 |  2800/ 2980 batches | lr 5.0 | ms/batch 4.5e+01 | loss   5.9 | ppl  3.8e+02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 1.4e+02s | valid loss   5.9 | valid ppl  3.6e+02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/ 2980 batches | lr 4.5 | ms/batch 4.8e+01 | loss   5.9 | ppl  3.6e+02\n",
            "| epoch   2 |   400/ 2980 batches | lr 4.5 | ms/batch 4.6e+01 | loss   5.8 | ppl  3.3e+02\n",
            "| epoch   2 |   600/ 2980 batches | lr 4.5 | ms/batch 4.6e+01 | loss   5.8 | ppl  3.4e+02\n",
            "| epoch   2 |   800/ 2980 batches | lr 4.5 | ms/batch 4.6e+01 | loss   5.7 | ppl    3e+02\n",
            "| epoch   2 |  1000/ 2980 batches | lr 4.5 | ms/batch 4.6e+01 | loss   5.7 | ppl    3e+02\n",
            "| epoch   2 |  1200/ 2980 batches | lr 4.5 | ms/batch 4.5e+01 | loss   5.7 | ppl    3e+02\n",
            "| epoch   2 |  1400/ 2980 batches | lr 4.5 | ms/batch 4.6e+01 | loss   5.6 | ppl  2.7e+02\n",
            "| epoch   2 |  1600/ 2980 batches | lr 4.5 | ms/batch 4.5e+01 | loss   5.6 | ppl  2.7e+02\n",
            "| epoch   2 |  1800/ 2980 batches | lr 4.5 | ms/batch 4.6e+01 | loss   5.6 | ppl  2.8e+02\n",
            "| epoch   2 |  2000/ 2980 batches | lr 4.5 | ms/batch 4.6e+01 | loss   5.7 | ppl  2.9e+02\n",
            "| epoch   2 |  2200/ 2980 batches | lr 4.5 | ms/batch 4.5e+01 | loss   5.6 | ppl  2.7e+02\n",
            "| epoch   2 |  2400/ 2980 batches | lr 4.5 | ms/batch 4.6e+01 | loss   5.7 | ppl  2.9e+02\n",
            "| epoch   2 |  2600/ 2980 batches | lr 4.5 | ms/batch 4.5e+01 | loss   5.7 | ppl  2.9e+02\n",
            "| epoch   2 |  2800/ 2980 batches | lr 4.5 | ms/batch 4.6e+01 | loss   5.6 | ppl  2.8e+02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 1.4e+02s | valid loss   5.8 | valid ppl  3.4e+02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 2980 batches | lr 4.3 | ms/batch 4.8e+01 | loss   5.6 | ppl  2.8e+02\n",
            "| epoch   3 |   400/ 2980 batches | lr 4.3 | ms/batch 4.6e+01 | loss   5.6 | ppl  2.6e+02\n",
            "| epoch   3 |   600/ 2980 batches | lr 4.3 | ms/batch 4.6e+01 | loss   5.6 | ppl  2.7e+02\n",
            "| epoch   3 |   800/ 2980 batches | lr 4.3 | ms/batch 4.6e+01 | loss   5.5 | ppl  2.5e+02\n",
            "| epoch   3 |  1000/ 2980 batches | lr 4.3 | ms/batch 4.6e+01 | loss   5.5 | ppl  2.4e+02\n",
            "| epoch   3 |  1200/ 2980 batches | lr 4.3 | ms/batch 4.6e+01 | loss   5.5 | ppl  2.5e+02\n",
            "| epoch   3 |  1400/ 2980 batches | lr 4.3 | ms/batch 4.7e+01 | loss   5.4 | ppl  2.3e+02\n",
            "| epoch   3 |  1600/ 2980 batches | lr 4.3 | ms/batch 4.5e+01 | loss   5.4 | ppl  2.2e+02\n",
            "| epoch   3 |  1800/ 2980 batches | lr 4.3 | ms/batch 4.6e+01 | loss   5.5 | ppl  2.4e+02\n",
            "| epoch   3 |  2000/ 2980 batches | lr 4.3 | ms/batch 4.5e+01 | loss   5.5 | ppl  2.4e+02\n",
            "| epoch   3 |  2200/ 2980 batches | lr 4.3 | ms/batch 4.6e+01 | loss   5.5 | ppl  2.4e+02\n",
            "| epoch   3 |  2400/ 2980 batches | lr 4.3 | ms/batch 4.6e+01 | loss   5.5 | ppl  2.5e+02\n",
            "| epoch   3 |  2600/ 2980 batches | lr 4.3 | ms/batch 4.6e+01 | loss   5.5 | ppl  2.5e+02\n",
            "| epoch   3 |  2800/ 2980 batches | lr 4.3 | ms/batch 4.6e+01 | loss   5.5 | ppl  2.4e+02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 1.4e+02s | valid loss   5.8 | valid ppl  3.2e+02\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWc_UXu0civk",
        "colab_type": "code",
        "outputId": "dcd07959-cd6b-4f5a-aa95-bea1c61a0be7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "test_loss = evaluate(best_model, test_data)\n",
        "print('=' * 89)\n",
        "print(f'| End of training | test loss {test_loss:5.2} | test ppl {math.exp(test_loss):8.2}')\n",
        "print('=' * 89)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss   5.7 | test ppl    3e+02\n",
            "=========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}