{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TEXT CLASSIFICATION WITH TORCHTEXT.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMns/4xYFSv/scCeDvQKAjk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/InhyeokYoo/PyTorch-tutorial-text/blob/master/TEXT_CLASSIFICATION_WITH_TORCHTEXT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6aYWsKh1QXQ",
        "colab_type": "text"
      },
      "source": [
        "# Intro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOAim9Y8s2iO",
        "colab_type": "text"
      },
      "source": [
        "이번 튜토리얼은 `torchtext` 내에 있는 text classification datasets을 어떻게 사용할 수 있는지 보여주고, 다음을 포함합니다.\n",
        "\n",
        "```\n",
        "- AG_NEWS,\n",
        "- SogouNews,\n",
        "- DBpedia,\n",
        "- YelpReviewPolarity,\n",
        "- YelpReviewFull,\n",
        "- YahooAnswers,\n",
        "- AmazonReviewPolarity,\n",
        "- AmazonReviewFull\n",
        "```\n",
        "\n",
        "이 예제는 이러한 `TextClassification` datasets 중 하나를 이용하여 지도학습 분류 알고리즘을 어떻게 학습시키는지 보여줍니다.\n",
        "\n",
        "> 지난번과 마찬가지로 본 튜토리얼은 파이토치 홈페이지 내의 튜토리얼을 번역한 자료입니다. 다수 의역이 되어있습니다. 역자주의 경우 지금과 같이 citation을 통해 남기도록 하겠습니다. 원문은 [다음](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html)과 같습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKOmAFsUtXxs",
        "colab_type": "text"
      },
      "source": [
        "# Load data with ngrams\n",
        "\n",
        "A bag of ngrams features는 국소적인(local) 단어 순서에 대한 특정한 정보를 사로잡기 위해 이용됩니다. 실전에선 bi-gram이나 tri-gram은 단어의 그룹으로서 제공되어 단순히 하나의 단어만 이용하는 것보다 더 유용한 정보를 제공해줍니다.\n",
        "\n",
        "```python\n",
        "\"load data with ngrams\"\n",
        "Bi-grams results: \"load data\", \"data with\", \"with ngrams\"\n",
        "Tri-grams results: \"load data with\", \"data with ngrams\"\n",
        "```\n",
        "\n",
        "`TextClassification` Dataset은 ngrams 메소드를 지원합니다. ngrams를 2로 세팅함으로써 dataset에 있는 예제 텍스트는 single words에 bi-grams string을 더한 list가 될 것입니다.\n",
        "\n",
        "> 시작하기에 앞서, 공식 튜토리얼과 colab이 제공하는 `torchtext`의 버전이 다르기 때문에 `torchtext`를 업그레이드 해주곘습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoDDfREsvNFr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "1fe3ff1b-989c-4b2d-ecfd-77fe2e6f52d8"
      },
      "source": [
        "!pip install --upgrade torchtext"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/ef/54b8da26f37787f5c670ae2199329e7dccf195c060b25628d99e587dac51/torchtext-0.5.0-py3-none-any.whl (73kB)\n",
            "\r\u001b[K     |████▌                           | 10kB 20.3MB/s eta 0:00:01\r\u001b[K     |█████████                       | 20kB 3.1MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 30kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 40kB 3.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 51kB 3.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 61kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 71kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 4.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.12.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\r\u001b[K     |▎                               | 10kB 23.2MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 31.1MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 37.7MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 42.7MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 46.0MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 49.6MB/s eta 0:00:01\r\u001b[K     |██▏                             | 71kB 11.1MB/s eta 0:00:01\r\u001b[K     |██▌                             | 81kB 12.2MB/s eta 0:00:01\r\u001b[K     |██▉                             | 92kB 13.4MB/s eta 0:00:01\r\u001b[K     |███▏                            | 102kB 14.6MB/s eta 0:00:01\r\u001b[K     |███▌                            | 112kB 14.6MB/s eta 0:00:01\r\u001b[K     |███▉                            | 122kB 14.6MB/s eta 0:00:01\r\u001b[K     |████                            | 133kB 14.6MB/s eta 0:00:01\r\u001b[K     |████▍                           | 143kB 14.6MB/s eta 0:00:01\r\u001b[K     |████▊                           | 153kB 14.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 163kB 14.6MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 174kB 14.6MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 184kB 14.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 194kB 14.6MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 204kB 14.6MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 215kB 14.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 225kB 14.6MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 235kB 14.6MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 245kB 14.6MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 256kB 14.6MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 266kB 14.6MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 276kB 14.6MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 286kB 14.6MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 296kB 14.6MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 307kB 14.6MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 317kB 14.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 327kB 14.6MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 337kB 14.6MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 348kB 14.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 358kB 14.6MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 368kB 14.6MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 378kB 14.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 389kB 14.6MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 399kB 14.6MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 409kB 14.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 419kB 14.6MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 430kB 14.6MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 440kB 14.6MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 450kB 14.6MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 460kB 14.6MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 471kB 14.6MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 481kB 14.6MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 491kB 14.6MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 501kB 14.6MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 512kB 14.6MB/s eta 0:00:01\r\u001b[K     |████████████████                | 522kB 14.6MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 532kB 14.6MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 542kB 14.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 552kB 14.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 563kB 14.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 573kB 14.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 583kB 14.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 593kB 14.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 604kB 14.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 614kB 14.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 624kB 14.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 634kB 14.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 645kB 14.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 655kB 14.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 665kB 14.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 675kB 14.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 686kB 14.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 696kB 14.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 706kB 14.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 716kB 14.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 727kB 14.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 737kB 14.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 747kB 14.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 757kB 14.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 768kB 14.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 778kB 14.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 788kB 14.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 798kB 14.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 808kB 14.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 819kB 14.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 829kB 14.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 839kB 14.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 849kB 14.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 860kB 14.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 870kB 14.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 880kB 14.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 890kB 14.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 901kB 14.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 911kB 14.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 921kB 14.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 931kB 14.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 942kB 14.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 952kB 14.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 962kB 14.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 972kB 14.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 983kB 14.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 993kB 14.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.0MB 14.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.0MB 14.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.0MB 14.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.0MB 14.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.0MB 14.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.28.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.17.5)\n",
            "Requirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.4.0)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "Successfully installed sentencepiece-0.1.85 torchtext-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alUW6r7utXQx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "4407370c-cc5d-4f67-c91d-08a78354bb7a"
      },
      "source": [
        "import torch\n",
        "import torchtext\n",
        "from torchtext.datasets import text_classification\n",
        "# from torchtext.datasets import AG_NEWS # 다른 방법으로 importing하기\n",
        "import os\n",
        "\n",
        "NGRAMS = 2 # final로 선언합니다.\n",
        "\n",
        "if not os.path.isdir('./.data'):\n",
        "    os.mkdir('./.data')\n",
        "train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n",
        "    root='./.data', ngrams=NGRAMS, vocab=None\n",
        ")\n",
        "BATCH_SIZE = 16\n",
        "device = torch.device('cpu' if torch.cuda.is_available() == False else 'cuda')\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ag_news_csv.tar.gz: 11.8MB [00:00, 69.6MB/s]\n",
            "120000lines [00:07, 16568.24lines/s]\n",
            "120000lines [00:14, 8253.64lines/s]\n",
            "7600lines [00:00, 8741.20lines/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMfH5tHC1Mv5",
        "colab_type": "text"
      },
      "source": [
        "# Define the model\n",
        "\n",
        "이 모델은 [EmbeddingBag](https://pytorch.org/docs/stable/nn.html?highlight=embeddingbag#torch.nn.EmbeddingBag) 레이어와 linear layer(밑에 그림 참고)로 이루어져 있습니다. `nn.EmbeddingBag`은 embedding 주머니(bag)의 평균값을 계산합니다. 튜토리얼의 text entries는 서로 다른 길이를 갖고 있습니다. `nn.EmbeddingBag`은 여기서 padding이 필요 없는데, 이는 offsets안에 텍스트의 길이가 저장되어 있기 때문입니다.\n",
        "\n",
        "추가적으로, `nn.EmbeddingBag`은 평균을 즉석에서 누적하므로, `nn.EmbeddingBag`은 tensor의 시퀀스를 다룰 때 메모리 효율과 성능을 향상시킬 수 있습니다.\n",
        "\n",
        "> `nn.EmbeddingBag`은 `nn.Embedding` 후 `torch.mean(dim=0)`와 동일합니다. 그러나, `EmbeddingBag`은 시간과 메모리면에서 훨씬 효율적입니다.\n",
        "Input은 `Input` (LongTensor), `offsets` (LongTensor, optional)으로, \n",
        "- 만일 `input`이 (*B*, *N*)으로 2D이면, 각 길이가 고정으로 `N`인 `B`개의 주머니 (sequence)로 취급합니다. 이는 `mode`에 따라 합산된 `B`개의 값을 반환할 것이며, `offsets`는 이 경우에 무시됩니다.\n",
        "- 만일 `input`이 (*N*)의 1D라면, 여러 주머니(sequences)의 concatenation으로 취급합니다. `offsets`는 1D tensor로, `input`안의 각 주머니가 시작하는 지점의 index를 포함합니다. 따라서, (*B*) 차원의 `offsets`에 대해서, `input`은 `B`개의 주머니를 갖는다고 할 수 있습니다. 빈 주머니(즉, 0짜리 길이)는 0으로 채워진 tensor를 반환합니다.\n",
        "\n",
        "![](https://pytorch.org/tutorials/_images/text_sentiment_ngrams_model.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9UQWYhFrriK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TextSentiment(nn.Module):\n",
        "    def __init__(self, vocab_size: int, embed_dim: int, num_class: int):\n",
        "        super(TextSentiment, self).__init__()\n",
        "        self.embedding = nn.EmbeddingBag(num_embeddings=vocab_size, embedding_dim=embed_dim, sparse=True)\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        return self.fc(embedded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7Csob3h8r7C",
        "colab_type": "text"
      },
      "source": [
        "# Initiate an instance\n",
        "\n",
        "AG_NEWS 데이터셋은 네 개의 레이블이있고, 따라서 클래스의 갯수는 4가 됩니다.\n",
        "```python\n",
        "1 : World\n",
        "2 : Sports\n",
        "3 : Business\n",
        "4 : Sci/Tec\n",
        "```\n",
        "\n",
        "사전의 크기(vocab size)는 사전(vocab; 개별 단어와 ngrams를 포함)의 길이와 같습니다. 클래스의 개수는 레이블의 개수와 같고, AG_NEWS의 경우 4가 됩니다.\n",
        "\n",
        "```python\n",
        "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
        "EMBED_DIM = 32\n",
        "NUN_CLASS = len(train_dataset.get_labels())\n",
        "model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCRTkeO-L1Pk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
        "EMBED_DIM = 32\n",
        "NUN_CLASS = len(train_dataset.get_labels())\n",
        "model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpYg56Tm9N5L",
        "colab_type": "text"
      },
      "source": [
        "# Functions used to generate batch\n",
        "\n",
        "다음 요소가 서로 다른 길이를 갖기때문에, 사용자 함수인 generate_batch()를 이용하여 data batches와 offsets를 생성합니다. 이 함수는 `torch.utils.data.DataLoader`안에 있는 `collate_fn`으로 전달됩니다. `collate_fn`의 인풋은 batch_size만큼의 크기를 갖는 tensors로 이루어진 list이고, `collate_fn` 함수는 mini_batch로 나눕니다. `collate_fn`은 최고 레벨의 함수로 선언되는 것에 주목합시다. 이는 이 함수가 각 worker에서 사용가능하게 합니다.\n",
        "\n",
        "원본 data batch input의 텍스트는 list로 감싸져있고, 하나의 tensor로 concat되어 `nn.EmbeddingBag`의 input이 됩니다. Offset은 text tensor내 개별 sequence의 시작점의 인덱스를 나타내는 텐서입니다.\n",
        "\n",
        ">  `torchtext.datasets.TextClassificationDataset`의 data에 대한 설명은 다음과 같습니다: label/tokens의 튜플로 이루어진 리스트로, tokens은 string tokens를 numericalizing한 것이고, label은 integer입니다. `[(label1, tokens1), (label2, tokens2), (label2, tokens3)]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4K6QH3h8q0o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_batch(batch):\n",
        "    label = torch.tensor([entry[0] for entry in batch])\n",
        "    text = [entry[1] for entry in batch]\n",
        "    offsets = [0] + [len(entry) for entry in text]\n",
        "    # torch.tensor.cumsum은 정해진 dimension dim\n",
        "    # 안에 요소의 누적합을 반환합니다.\n",
        "    # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\n",
        "\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text = torch.cat(text)\n",
        "    return text, offsets, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVOcppfSE7QS",
        "colab_type": "text"
      },
      "source": [
        "# Define functions to train the model and evaluate results.\n",
        "`torch.utils.data.DataLoader`는 손쉽게 병렬 데이터 로딩을 가능케하고 PyTorch 사용자에게 권장됩니다. 여기서는 `DataLoader`를 사용하여 AG_NEWS 데이터셋을 불러오고 모델로 보내 training과 validation을 해보도록 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmQQOeU5E6sZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train_func(sub_train_):\n",
        "    # Train the model\n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "\n",
        "    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
        "    for i, (text, offsets, cls) in enumerate(data):\n",
        "        optimizer.zero_grad()\n",
        "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
        "        output = model(text, offsets)\n",
        "        loss = criterion(output, cls)\n",
        "        train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # 마지막엔 softmax layer를 추가해도 좋습니다.\n",
        "        train_acc += (output.argmax(1) == cls).sum().item()\n",
        "\n",
        "    # Learning rate 조정\n",
        "    scheduler.step()\n",
        "\n",
        "    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n",
        "\n",
        "def test(data_):\n",
        "    loss = 0\n",
        "    acc = 0\n",
        "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
        "\n",
        "    for text, offsets, cls in data:\n",
        "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(text, offsets)\n",
        "            loss = criterion(output, cls)\n",
        "            loss += loss.item()\n",
        "            # 마지막엔 softmax layer를 추가해도 좋습니다.\n",
        "            acc += (output.argmax(1) == cls).sum().item()\n",
        "\n",
        "    return loss / len(data_), acc / len(data_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfgJ-mafFTLK",
        "colab_type": "text"
      },
      "source": [
        "> 살짝 헷갈리므로 불러오는 데이터의 형식을 한번 살펴보도록 하겠습니다. Tractable 하기위해 `shuffle=False`로 설정하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_01VNluFRtl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2fe964a1-5d33-4ee6-999e-991f6781377f"
      },
      "source": [
        "data = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=generate_batch)\n",
        "for i, (text, offsets, cls) in enumerate(data):\n",
        "    print(f\"text: {text}, len: {len(text)}\")\n",
        "    print(f\"offsets: {offsets}, length:{len(offsets)}\")\n",
        "    print(f\"cls: {cls}, len: {len(cls)}\")\n",
        "    print(\"=\"*200)\n",
        "    if i == 3:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "text: tensor([    572,     564,       2,  ..., 1194110,  300136,   10278]), len: 1432\n",
            "offsets: tensor([   0,   57,  140,  219,  298,  383,  478,  571,  668,  843,  904,  991,\n",
            "        1102, 1163, 1262, 1369]), length:16\n",
            "cls: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), len: 16\n",
            "========================================================================================================================================================================================================\n",
            "text: tensor([   6312,    1934,      12,   11087,       3,   22219,     518,       7,\n",
            "           6312,      24,       6,     330,     483,       7,    1616,       3,\n",
            "          11087,     160,      33,     805,     749,       6,    1662,    2240,\n",
            "          32572,       2,  902849,   75962,   98359,  607543,  285041, 1066517,\n",
            "          15746,   75193,  902829,     748,    2338,   86787,     711,   90638,\n",
            "          13928,   92801,   58329,   43326,    3790,  326457,    9257,    3783,\n",
            "         854300,  941058,  133813,       8,       6,     185,     172,       4,\n",
            "            491,    1150,    2629,    1675,     311,      24,     729,    2347,\n",
            "             12,    8779,       8,       3,     397,     172,    1772,     406,\n",
            "              4,      55,     484,       8, 1264126,    3234,    1675,      49,\n",
            "           3409,       5,    4556,       6,    3972,  205647,      67,     213,\n",
            "            389,     409,      90,      17,    2199,     807,     961,      95,\n",
            "              3,   12710,      99,   10167,      20,     339,       2,      69,\n",
            "          95020,  736002,    2851,   17829,  374787, 1237903, 1264092,  805618,\n",
            "           2747,   68653,  257462,   34483,  186016,  108979,      26,    5641,\n",
            "           4893,  398378,    9078,    4636,     118,   58487,   21111,  865376,\n",
            "        1264127,    8199,  128097,   74027,    9735,   30707,  110726,    5686,\n",
            "         628300, 1153763,   20034,   49689,   90771,    8704,    4062,    5581,\n",
            "          93577,  929207,    1867,    3013,   50849,  110950, 1287945,  164313,\n",
            "         213522,    6660,      45,     365,    1742,   83790,       8,    2850,\n",
            "              3,      45,     365,    1742,      33,    2741,    1202,       5,\n",
            "              6,     198,    6510,       2,   30845,      21,      78,     708,\n",
            "           4155,    4997,     379,       4,     287,       5,       6,     415,\n",
            "           2134,       2,   12477,    6830,  109897, 1189362,    8652,  388782,\n",
            "            243,   12477,    6830,  237651,   55629,  765573,   71724,     120,\n",
            "           1846,   91472,   34265,   36919,  534938,    2339,   34997,  125739,\n",
            "         739261,  861166,   10875,     356,     292,     120,  220766,  100118,\n",
            "          24234,    4793,      17,     125,      47,    1045,      12,    1852,\n",
            "             17,      78,     334,    4793,     125,      47,   25346,    2290,\n",
            "             12,       6,     938,    1374,       4,    4239,      34,     573,\n",
            "            622,    1852,       4,       6,     445,     201,     763,       2,\n",
            "         205030,   77575,     893,  325961,   12783,   88414, 1236518,  169373,\n",
            "           4176,   59244,  443323,     893,  325695,  230304,   47142,     159,\n",
            "          10752,   60980,   62410,   14159, 1049256,   88523,  245327, 1101491,\n",
            "         210559,      87,   11236,   60403,  432179,   29108,     271,    2394,\n",
            "           1278,   16001,  117357,       3,    7356,     714,    9982,      12,\n",
            "            271,      17,      10,     513,     978,       4,     467,   10354,\n",
            "           1764,      48,      35,    2657,      20,      25,   16676,       8,\n",
            "          16001,    3161,       2,   17671,  880443,  768722, 1042460, 1151655,\n",
            "          29673,  625374,   59242,   84515,   26125,    9219,      23,   31602,\n",
            "           3878,   16807,    2405,  723052,  910659,   15083,   11985,    6514,\n",
            "          12892,    1769,  386528,   96424,  251636,  101870,   11022,   10331,\n",
            "            469,    2864,    1097,     396,    2134,     684,       3,   94648,\n",
            "          10331,     469,    1668,       5,    3091,       4,      55,     311,\n",
            "             52,    4479,      30,     137,    1785,     185,    1508,       8,\n",
            "              3,      94,       2,   80023,  745580,  192332,  825782,   22182,\n",
            "          31177,   16039,  103962,  218068,   80023,  110300,    3850,    6498,\n",
            "          59288,     118,    6605,    3430,  320282,  291116,    3810,   32452,\n",
            "          27156,  735987,   12904,      26,    1011,     682,    2509,    8597,\n",
            "              8,     259,     489,     339,       8,     259,    8597,     185,\n",
            "             21,       3,     244,   24219,       6,     902,       8,    2418,\n",
            "              9,    1035,    1082,       2,  764286,   56910,    2320,  891739,\n",
            "           2262,    2410,    2320,  891913,   92233,    7696,     194,     428,\n",
            "         700072,  184707,    7882,    2372,   63662,   46665,   16655,   87467,\n",
            "          15504,   28692,    1661,      11,    3575,    5737,     873,     374,\n",
            "            782,     818,      52,   22854,       5,     412,       2,     336,\n",
            "             28,       3,     289,    1059,     745,     255,       4,      55,\n",
            "              3,    3605,       7,    1282,     840,       3,   28692,       9,\n",
            "          17783,     975,       2, 1076001,   18610, 1006034, 1141777, 1114946,\n",
            "         429101,  708148,    1683,   64842,  590747,  288299,   14131,    2836,\n",
            "            854,  219269,     207,    3689,    4348,  557636,    3740,    4170,\n",
            "            118,     837,   19174,    4251,  264657,  474106,   22593,  137995,\n",
            "        1075998,  317874, 1187133,    5961,     493,     124,     185,     799,\n",
            "              3,     678,       3,    1064,       7,    1879,     643,      27,\n",
            "              9,     173,    1649,    2531,     582,    1896,      48,       3,\n",
            "            476,     335,     130,       4,       6,      27,    1206,      33,\n",
            "            443,       2,  147011,   60410,  352073,    1630,    4311,   46421,\n",
            "           5839,    4281,   47430,  649660,  146232,   16528,   32011, 1128045,\n",
            "         830772,  660740,   15668,  277998,     204,    1286,   45200,    3633,\n",
            "           1255,      87,     151,   21172,   83783,    6615,   12878,     289,\n",
            "            557,    8963,     782,     818,     289,     557,      17,      10,\n",
            "            745,     255,     738,     782,     818,      28,       6,     353,\n",
            "           9845,    1310,       5,     297,       2,     336,       8,       6,\n",
            "            307,       5,    1190,     339,       8,       3,     469,       2,\n",
            "           2148,  905803,  930413,    1683, 1077451,    2148,    8051,      23,\n",
            "          10711,    3740,  227697,  708442,    1683,   27084,     514,    8765,\n",
            "          82786,   27053,   11989,   17216,    2251,     854,   26601,      69,\n",
            "           2630,    1324,    8524,  149492,    2410,      26,    1877,    4261,\n",
            "            271,    2325,    2081,      11,      86,      35,    2325,       7,\n",
            "            249,       8,     271,       4,       3,     305,     238,     985,\n",
            "            136,     125,      47,   23622,      12,      21,     519,      21,\n",
            "         307773,       4,     720,     780,      11,      86,       2,  819593,\n",
            "         602813,   51767,     193,  111243,   20497,   16668,   16534,    4002,\n",
            "          28481,    9596,      42,    2628,    5551,    1269,  355827,    6262,\n",
            "            893,  615034,  786442,   24240,    2639,    3163,  593794,  538085,\n",
            "          25882,   48015,   15000,     193,     558,    1494,     249,   10616,\n",
            "             11,     209,     186,    2771,     249,     566,      40,    3622,\n",
            "            785,     883,       4,     304,       3,     436,    1522,       3,\n",
            "            295,     353,      39,     604,     566,    1292,       7,    1357,\n",
            "              2,  853804,   60759,   84464,   30434,  425407,  407335,  249303,\n",
            "          11179,   34645,   48710,   87754,   15807,   12436,    1027,    2878,\n",
            "           5128,  243396,   84693,    1012,  150771,  102202,   16093,  314939,\n",
            "          24227,    4861,   39375,    6612,  944226,    9335,     436,     738,\n",
            "            447,      82,       7,       3,   10701,    9335,    5251,      11,\n",
            "              3,     948,    5250,    1382,       7,  399086,      92,     113,\n",
            "           3023,     623,    6152,       9,     374,   10187,     447,       2,\n",
            "         944227, 1203085,  782156,   62895,  894646,     283,      29,   20845,\n",
            "        1003097, 1203094, 1011019,      79,    6102,   47059,  263709,    9048,\n",
            "         995233,  944228,     605,  475382,  444192, 1137012,   52209,   14611,\n",
            "          73146,   25909,   10684,    6180,    1369,    3755,     886,      34,\n",
            "           5465,    6180,    3587,       3,    5465,      12,       6,    5923,\n",
            "              5,     477,      12,    1112,      40,      56,      95,    1009,\n",
            "              4,     219,    5474,      34,  181073,     614,       8,    1115,\n",
            "            722,       2,  665893,  277044, 1085426,   58078,  366974,  860022,\n",
            "         665870,   51665,   41643,  379955,     159,  106665,   37664,    1197,\n",
            "           7534,  244361,  257785,   14174,     192,    6215,   10100,     223,\n",
            "         106117,   82914,  801917,  690986,   55184,    6531,   20894,   25114,\n",
            "            259,     378,     436,   12450,    4612,       3,      72,     802,\n",
            "              3,     647,     378,    1821,     313,      28,       6,   10120,\n",
            "           5153,      24,       5,     551,      25,   53338,      12,    1456,\n",
            "           6466,       2,  387544,  989428,  362050, 1144616, 1041916,     202,\n",
            "         686927,   16025,    3967,  130283,   56299, 1041765,    3822,     514,\n",
            "          43934,  242516,  550834,    1451,    4543,   26753,  386890, 1079358,\n",
            "          80327,  116774,   96841,    3043,   18284,       8,     172,   12708,\n",
            "          43629,  614261,       4,       3,    3043,   18284,    5393,      12,\n",
            "           4104,       3,  799810,  339840,     939,       4,      24,    2129,\n",
            "              5,   12708,      38,      72,      11,       3,     397,     172,\n",
            "              2,  472271,  878834,   25512,  940622,  786434, 1242042,  614262,\n",
            "             42,   20849,  472271,  878830,   59022,   59117,   18963, 1209783,\n",
            "         799811,  678659,   30520,     734,    7373,    4830,   57284,  786418,\n",
            "           9712,   29117,      79,    5641,    4893,    1818]), len: 966\n",
            "offsets: tensor([  0,  51, 154, 217, 278, 335, 390, 433, 500, 559, 624, 685, 740, 795,\n",
            "        856, 907]), length:16\n",
            "cls: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), len: 16\n",
            "========================================================================================================================================================================================================\n",
            "text: tensor([   2226,    4547,       5,  ..., 1083638,   24519,   11559]), len: 1552\n",
            "offsets: tensor([   0,   71,  138,  189,  252,  353,  450,  587,  738,  907, 1010, 1077,\n",
            "        1196, 1321, 1402, 1467]), length:16\n",
            "cls: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), len: 16\n",
            "========================================================================================================================================================================================================\n",
            "text: tensor([   12,  1001,     3,  ..., 53174,    37,    37]), len: 1296\n",
            "offsets: tensor([   0,   75,  110,  151,  208,  271,  354,  403,  440,  479,  522,  641,\n",
            "         818,  911, 1078, 1187]), length:16\n",
            "cls: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), len: 16\n",
            "========================================================================================================================================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IETD8bVIFtP0",
        "colab_type": "text"
      },
      "source": [
        "> 이로 미루어 알 수 있는 사실은 16개 (`len(offsets)` or `len(cls)`)의 sequences (bags)가 text 안에 담겨있고, offsets는 각 bag의 시작점을 알려주고 있습니다. 각 bag의 길이는 모두 서로 다릅니다. 본 데이터 셋은 [Batch x length]를 반환하는 대신, [BATCH_SIZE] concat된 bag과 offsets를 반환합니다.  \n",
        "참고로 원래의 dataset은 다음과 같이 생겼습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J2HypMHE59x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "outputId": "8dd25e3b-50d6-4fae-be71-c6c30d128456"
      },
      "source": [
        "data = DataLoader(train_dataset, shuffle=False)\n",
        "for i, (x) in enumerate(data):\n",
        "    print(x, x[1].size())\n",
        "    print(\"=\"*200)\n",
        "    if i == 3:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[tensor([2]), tensor([[    572,     564,       2,    2326,   49106,     150,      88,       3,\n",
            "            1143,      14,      32,      15,      32,      16,  443749,       4,\n",
            "             572,     499,      17,      10,  741769,       7,  468770,       4,\n",
            "              52,    7019,    1050,     442,       2,   14341,     673,  141447,\n",
            "          326092,   55044,    7887,     411,    9870,  628642,      43,      44,\n",
            "             144,     145,  299709,  443750,   51274,     703,   14312,      23,\n",
            "         1111134,  741770,  411508,  468771,    3779,   86384,  135944,  371666,\n",
            "            4052]])] torch.Size([1, 57])\n",
            "========================================================================================================================================================================================================\n",
            "[tensor([2]), tensor([[  55003,    1474,    1150,    1832,    7559,      14,      32,      15,\n",
            "              32,      16,    1262,    1072,     436,   55003,     131,       4,\n",
            "          142576,      33,       6,    8062,      12,     756,  475640,       9,\n",
            "          991346,    3186,       8,       3,     698,     329,       4,      33,\n",
            "            6764, 1040465,   13979,      11,     278,     483,       7,       3,\n",
            "             172,       2,  659973,  193730, 1237754,  684719,  556644,      43,\n",
            "              44,     144,     145,   77775,   56578,   32382,  782124,   79225,\n",
            "            2908,  140697,  540900,    2031,   31960,   45339,   21562,  936430,\n",
            "         1282186,  578442,  991347,   69671,      26,    9260,  717285,    5378,\n",
            "             597,   27622, 1070413, 1040466,   38669,   27790,  175394,     711,\n",
            "              29,    1404,    1818]])] torch.Size([1, 83])\n",
            "========================================================================================================================================================================================================\n",
            "[tensor([2]), tensor([[    78,      9,    469,   8385,    206,     17,    996,     14,     32,\n",
            "             15,     32,     16,   3654,    600,    124,   3080, 140201,      3,\n",
            "            469,      9,      3,    996,     12,    369,     52,    328, 464765,\n",
            "             48,      3,    397,    172,    149,    113,    301,      3,  18223,\n",
            "              7, 285456,  52106,      2,   5606,  95553, 183737, 180431, 164136,\n",
            "         120431,  30446,     43,     44,    144,    145,  85276,  92253,  10654,\n",
            "         200704, 422054, 213900,   1877,   8549,    152,  12306,   6126,  55468,\n",
            "          63058,   4461, 358165, 464766,    204,   5641,   4893,  59857,   1515,\n",
            "          93728,    995,  83994,  79735, 411437, 460552, 110113]])] torch.Size([1, 79])\n",
            "========================================================================================================================================================================================================\n",
            "[tensor([2]), tensor([[     93,   16478,      78,    2680,      34,    1230,     717,    4562,\n",
            "              14,      32,      15,      32,      16,    1129,      49,    9392,\n",
            "              78,  766148,      34,       3,    1230,    4562,       8,     717,\n",
            "              93,  559272,     947,       6,    1239,    3934,     125, 1178056,\n",
            "               4,      35,      78,     396,      31,      11,     153,       2,\n",
            "          881320,  373252,   12125,   23059,  802619,  258328,  447615,  199864,\n",
            "              43,      44,     144,     145,   40078,   13205,  152671,  373240,\n",
            "         1002054,  766149,     171,    4412,  934044,   47573,    2586,   17490,\n",
            "          881151,  559273,   16579,   17268,   64855,  954751,  699138, 1178057,\n",
            "             733,   26650,   36317,    1670,     177,     435,     949]])] torch.Size([1, 79])\n",
            "========================================================================================================================================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Af6_pWH4cSq",
        "colab_type": "text"
      },
      "source": [
        "> 모든 데이터를 다 확인할 순 없지만 앞서 `collate_fn`을 이용한 결과와 위의 결과를 비교하면 `BATCH_SIZE`만큼 데이터를 concat하여 사용함을 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1QjfFUyChCt",
        "colab_type": "text"
      },
      "source": [
        "# Split the dataset and run the model\n",
        "원본 AG_NEWS 데이터에는 validation set이 없으므로 training set을 0.95의 비율로 나누겠습니다. 이를 위해 우리는 [torch.utils.data.datasets.random_split](https://pytorch.org/docs/stable/data.html?highlight=random_split#torch.utils.data.random_split)을 사용하겠습니다.\n",
        "\n",
        "[CrossEntropyLoss](https://pytorch.org/docs/stable/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss)는 nn.LogSoftmax()와 nn.NLLLoss()를 하나로 합쳐놓은 것입니다. 이는 C개의 클래스를 분류하는 문제를 학습시킬 때 유용합니다. [SGD](https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html)는 stochastic gradient descent를 optimizer로서 구현한 것입니다. 초기 learning rate값은 4.0으로 설정되어있습니다. [StepLR](https://pytorch.org/docs/master/_modules/torch/optim/lr_scheduler.html#StepLR)은 여기서 epochs에 따라 learning rate를 조절하기 위해 사용되었습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tclX26324Q29",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "2089096e-1a6a-4079-d165-fee07b549336"
      },
      "source": [
        "import time\n",
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "N_EPOCHS = 5\n",
        "min_valid_loss = float('inf')   # 왜 있는지 모르겠음\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n",
        "# StepLR은 step_size마다 lr을 gamma만큼 줄여서 작동한다: 이 경우, 4.0, 3.6, ..., 로 줄어든다.\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=1, gamma=0.9)\n",
        "\n",
        "train_len = int(len(train_dataset) * 0.95)\n",
        "# random_split(dataset: Dataset, lengths: 'sequence')\n",
        "# Arguments:\n",
        "#        dataset (Dataset): Dataset to be split\n",
        "#        lengths (sequence): lengths of splits to be produced\n",
        "sub_train_, sub_valid_ = random_split(dataset=train_dataset, lengths=[train_len, len(train_dataset) - train_len])\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = train_func(sub_train_)\n",
        "    valid_loss, valid_acc = test(sub_valid_)\n",
        "\n",
        "    secs = int(time.time() - start_time)\n",
        "    mins = secs / 60\n",
        "    secs = secs % 60\n",
        "\n",
        "    # f string\n",
        "    print(f'Epoch: {epoch + 1}', f\" | time in {mins} minutes, {secs} seconds\")\n",
        "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
        "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1  | time in 0.13333333333333333 minutes, 8 seconds\n",
            "\tLoss: 0.0126(train)\t|\tAcc: 93.3%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 93.2%(valid)\n",
            "Epoch: 2  | time in 0.13333333333333333 minutes, 8 seconds\n",
            "\tLoss: 0.0071(train)\t|\tAcc: 96.2%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 93.1%(valid)\n",
            "Epoch: 3  | time in 0.13333333333333333 minutes, 8 seconds\n",
            "\tLoss: 0.0039(train)\t|\tAcc: 98.1%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 93.0%(valid)\n",
            "Epoch: 4  | time in 0.13333333333333333 minutes, 8 seconds\n",
            "\tLoss: 0.0023(train)\t|\tAcc: 98.9%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 93.4%(valid)\n",
            "Epoch: 5  | time in 0.13333333333333333 minutes, 8 seconds\n",
            "\tLoss: 0.0014(train)\t|\tAcc: 99.4%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 93.8%(valid)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nIbV2HsRiLT",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate the model with test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNjkEbxNP53b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bc47a1f4-a49c-46f7-89b9-e94a405d3dc2"
      },
      "source": [
        "print('Checking the results of test dataset...')\n",
        "test_loss, test_acc = test(test_dataset)\n",
        "print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking the results of test dataset...\n",
            "\tLoss: 0.0003(test)\t|\tAcc: 90.5%(test)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwl6DCEQRny1",
        "colab_type": "text"
      },
      "source": [
        "# Test on a random news\n",
        "제일 좋은 모델을 사용하여 골프 뉴스를 테스트해보자. label정보는 [이곳](https://pytorch.org/text/datasets.html?highlight=ag_news#torchtext.datasets.AG_NEWS)에서 확인할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgqlRKawRkYC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f030439c-8cb8-47a4-95d6-c29f3a7b7da2"
      },
      "source": [
        "from torchtext.data.utils import ngrams_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "ag_news_label = {1 : \"World\",\n",
        "                 2 : \"Sports\",\n",
        "                 3 : \"Business\",\n",
        "                 4 : \"Sci/Tec\"}\n",
        "\n",
        "def predict(text, model, vocab, ngrams):\n",
        "    tokenizer = get_tokenizer(\"basic_english\")\n",
        "    with torch.no_grad():\n",
        "        text = torch.tensor([vocab[token]\n",
        "                            for token in ngrams_iterator(tokenizer(text), ngrams)])\n",
        "        output = model(text, torch.tensor([0]))\n",
        "        return output.argmax(1).item() + 1\n",
        "\n",
        "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
        "    enduring the season’s worst weather conditions on Sunday at The \\\n",
        "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
        "    considering the wind and the rain was a respectable showing. \\\n",
        "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
        "    was another story. With temperatures in the mid-80s and hardly any \\\n",
        "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
        "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
        "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
        "    was even more impressive considering he’d never played the \\\n",
        "    front nine at TPC Southwind.\"\n",
        "\n",
        "vocab = train_dataset.get_vocab()\n",
        "model = model.to(\"cpu\")\n",
        "\n",
        "print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, model, vocab, 2)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a Sports news\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8aW4xCbR0Yj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}